# Initiative 2.1: Writer Pooling & Background Flushing

**Status**: ðŸš§ In Progress
**Goal**: Fix consume issue + improve write throughput from ~150 rps to 1,000+ rps
**Timeline**: Week 9

## Problem

Phase 1 has a critical limitation:
- Each `produce()` call creates a new `SegmentWriter`
- Writer is dropped immediately after write
- Segment stays in memory, never flushed to storage
- Consumer can't read records because they're not persisted

**Result**: Consume operations fail with "Offset not found"

## Solution Architecture

### 1. Writer Pool

Keep one active `SegmentWriter` per partition:

```rust
pub struct WriterPool {
    // Key: (topic, partition)
    writers: Arc<RwLock<HashMap<(String, u32), Arc<Mutex<SegmentWriter>>>>>,
    storage_backend: Arc<dyn StorageBackend>,
    metadata_store: Arc<MetadataStore>,
}

impl WriterPool {
    pub async fn get_writer(&self, topic: &str, partition: u32) -> Result<Arc<Mutex<SegmentWriter>>> {
        // Return existing writer or create new one
    }

    pub async fn flush_all(&self) -> Result<()> {
        // Flush all active writers to storage
    }
}
```

### 2. Background Flush Thread

Periodically flush active segments:

```rust
tokio::spawn(async move {
    let mut interval = tokio::time::interval(Duration::from_secs(5));
    loop {
        interval.tick().await;
        if let Err(e) = writer_pool.flush_all().await {
            eprintln!("Background flush failed: {}", e);
        }
    }
});
```

**Flush triggers**:
- Every 5 seconds (configurable)
- Segment reaches max size (256 KB)
- Server shutdown (graceful flush)

### 3. Modified Produce Handler

```rust
async fn handle_produce(
    writer_pool: Arc<WriterPool>,
    request: ProduceRequest,
) -> Result<ProduceResponse> {
    let writer = writer_pool.get_writer(&request.topic, request.partition).await?;

    let mut writer_guard = writer.lock().await;
    let offset = writer_guard.append(record)?;

    // Check if segment should roll
    if writer_guard.should_roll() {
        writer_guard.flush().await?;
        // New writer will be created on next request
    }

    Ok(ProduceResponse { offset, timestamp })
}
```

## Benefits

1. **Fixes consume** - Segments are flushed periodically and available for reading
2. **Better throughput** - Reusing writers eliminates setup/teardown overhead
3. **Batching** - Multiple records can share the same segment
4. **Graceful shutdown** - Flush on SIGTERM ensures no data loss

## Implementation Plan

### Step 1: Create WriterPool struct
- `crates/streamhouse-storage/src/writer_pool.rs`
- `get_writer()` method with lazy creation
- `flush_all()` method

### Step 2: Integrate into server
- Add `WriterPool` to server state
- Modify produce handler to use pool
- Start background flush thread

### Step 3: Graceful shutdown
- Handle SIGTERM signal
- Flush all writers before exit
- Update metadata with final offsets

### Step 4: Testing
- Integration test: produce â†’ consume works
- Test: background flush persists data
- Test: graceful shutdown doesn't lose data

## Configuration

New environment variables:
```bash
# Flush interval in seconds (default: 5)
FLUSH_INTERVAL_SECS=5

# Max segment size before auto-flush (default: 256KB)
MAX_SEGMENT_SIZE_BYTES=262144
```

## Expected Performance

**Before** (Phase 1):
- Write throughput: ~150 rps
- Consume: Broken (segments not flushed)

**After** (Phase 2.1):
- Write throughput: ~1,000 rps (6x improvement)
- Consume: Works (background flush)
- Latency: ~5ms avg (down from ~8ms)

## Risks

1. **Memory pressure** - Active writers hold segments in memory
   - Mitigation: Max 1 writer per partition, flush frequently

2. **Concurrent access** - Multiple threads accessing same writer
   - Mitigation: Use `Arc<Mutex<SegmentWriter>>` for thread safety

3. **Flush failures** - S3 upload errors could lose data
   - Mitigation: Retry logic + keep segment in memory until confirmed

## Next Steps

After this initiative:
- 2.2: Kafka Protocol (Weeks 10-12)
- **Phase 3: Scalable Metadata (Weeks 13-20)** ðŸŽ¯ **NEW PRIORITY**
  - See [WARPSTREAM-LEARNINGS.md](../WARPSTREAM-LEARNINGS.md) for details
  - Pluggable metadata backend (PostgreSQL/CockroachDB/DynamoDB)
  - In-memory caching layer
  - Partitionâ†’segment index optimization
  - Cross-partition batching
  - **Why this matters**: WarpStream's success came from their hyper-scalable metadata service. This is mandatory before multi-agent architecture.

## Success Criteria

- âœ… Consume works immediately after produce
- âœ… Write throughput > 1,000 rps
- âœ… All integration tests pass
- âœ… Graceful shutdown doesn't lose data
- âœ… Background flush runs every 5 seconds

---

*Started: 2025-01-22*
