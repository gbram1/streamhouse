# Initiative 1.2: Segment Writer

**Timeline:** Week 1-2
**Goal:** Design and implement the core data format for storing events efficiently in S3

---

## Overview

The **Segment Writer** is the foundational component responsible for taking individual records (events/messages) and packaging them into efficient, compressed files that get uploaded to S3. It's the core of the storage layer and determines the performance, cost, and reliability of the entire system.

---

## The Problem

### Why We Need a Segment Writer

When streaming events at high volume, you face a critical challenge:

**Naive Approach (DON'T DO THIS):**
```
Event 1 arrives → Upload to S3 → Done
Event 2 arrives → Upload to S3 → Done
Event 3 arrives → Upload to S3 → Done
... 10,000 times per second ...
```

**Problems:**
- ❌ **Cost**: S3 charges ~$0.005 per 1,000 PUT requests
  - At 10,000 events/sec = 864 million PUTs/day
  - Cost: $4,320/day just in PUT costs!
- ❌ **Performance**: Each upload has network latency (50-200ms)
  - Can't achieve high throughput
- ❌ **No Compression**: Individual small records can't be compressed efficiently
- ❌ **Read Inefficiency**: To read 1000 records = 1000 S3 GET requests

**Our Approach (Segment Writer):**
```
Collect 1000s of events in memory
  ↓
Batch into blocks (~1MB each)
  ↓
Compress each block (LZ4)
  ↓
Build index for fast lookups
  ↓
Write single file to S3 (64-256MB)
```

**Benefits:**
- ✅ **Cost**: 10,000 events/sec with 64MB segments = ~150 PUTs/day = $0.0007/day (99.98% savings)
- ✅ **Performance**: Batch writes = minimal network overhead
- ✅ **Compression**: 80%+ space savings (JSON → LZ4 compressed blocks)
- ✅ **Fast Reads**: Index enables O(log n) offset lookups

---

## Segment File Format

### Structure

```
┌─────────────────────────────────────────────────────────────┐
│                    SEGMENT FILE                             │
│                 (64MB - 256MB typical size)                 │
├─────────────────────────────────────────────────────────────┤
│  HEADER (64 bytes)                                          │
│  ├── Magic (4 bytes): "STRM"          ← File type marker   │
│  ├── Version (2 bytes): 1             ← Format version     │
│  ├── Flags (2 bytes): compression     ← LZ4/Zstd/None      │
│  ├── Topic Hash (8 bytes)             ← Which topic        │
│  ├── Partition (4 bytes)              ← Which partition    │
│  ├── Base Offset (8 bytes)            ← First offset       │
│  ├── Max Offset (8 bytes)             ← Last offset        │
│  ├── Record Count (4 bytes)           ← Total records      │
│  ├── Created At (8 bytes)             ← Timestamp          │
│  ├── Min Timestamp (8 bytes)          ← Earliest event     │
│  └── Max Timestamp (8 bytes)          ← Latest event       │
├─────────────────────────────────────────────────────────────┤
│  BLOCK 1 (~1MB uncompressed → ~256KB compressed)           │
│  ├── Block Header (16 bytes)                               │
│  │   ├── Uncompressed Size (4 bytes)                       │
│  │   ├── Compressed Size (4 bytes)                         │
│  │   ├── Record Count (4 bytes)                            │
│  │   └── CRC32 (4 bytes)              ← Corruption check   │
│  └── Compressed Records                                    │
│      Record 0: [offset_delta][ts_delta][key_len][key]...   │
│      Record 1: [offset_delta][ts_delta][key_len][key]...   │
│      ... 500 more records ...                               │
├─────────────────────────────────────────────────────────────┤
│  BLOCK 2 (~1MB uncompressed)                               │
│      Record 500: ...                                        │
│      ... 500 more records ...                               │
├─────────────────────────────────────────────────────────────┤
│  ... more blocks ...                                        │
├─────────────────────────────────────────────────────────────┤
│  INDEX (enables fast offset lookups)                        │
│  ├── Entry Count (4 bytes)                                 │
│  └── Entries: [Offset (8) + Position (8)] × N             │
│      Entry 1: offset 0 → byte position 64                  │
│      Entry 2: offset 500 → byte position 256,000           │
│      Entry 3: offset 1000 → byte position 512,000          │
├─────────────────────────────────────────────────────────────┤
│  FOOTER (32 bytes)                                          │
│  ├── Index Offset (8 bytes)           ← Where index starts │
│  ├── Index Size (4 bytes)             ← Index byte size    │
│  ├── File CRC32 (4 bytes)             ← Full file check    │
│  ├── Reserved (12 bytes)              ← Future use         │
│  └── Magic (4 bytes): "MRTS"          ← End marker         │
└─────────────────────────────────────────────────────────────┘
```

### Record Format (within blocks)

Records use **variable-length encoding** and **delta compression**:

```
Record =>
  ├── Offset Delta (varint)      ← offset - block_base_offset
  ├── Timestamp Delta (varint)   ← timestamp - block_base_timestamp
  ├── Key Length (varint)        ← -1 for null
  ├── Key (bytes)                ← if length >= 0
  ├── Value Length (varint)      ← message size
  └── Value (bytes)              ← actual message data
```

**Why deltas?**
- Sequential offsets: `1000000, 1000001, 1000002` → stored as `1000000, +1, +1`
- Sequential timestamps: `1642534800000, 1642534800100, 1642534800200` → `1642534800000, +100, +100`
- Varints encode small numbers in 1-2 bytes instead of always using 8 bytes
- **Result**: 5-10x size reduction before compression even starts!

---

## How the Segment Writer Works

### Lifecycle

```rust
// 1. CREATE a new segment
let mut writer = SegmentWriter::new(
    topic: "orders",
    partition: 0,
    base_offset: 0
);

// 2. APPEND records (happens in memory, very fast)
writer.append(
    key: Some(b"user-123"),
    value: b"{\"order_id\": 1, \"amount\": 50.00}",
    timestamp: 1642534800000
);  // Returns offset 0

writer.append(
    key: Some(b"user-456"),
    value: b"{\"order_id\": 2, \"amount\": 75.00}",
    timestamp: 1642534800100
);  // Returns offset 1

// ... thousands more appends ...

// 3. FINISH when segment is full (size or time threshold)
let segment_bytes: Bytes = writer.finish();

// 4. UPLOAD to S3
upload_to_s3("data/orders/0/00000000000000000000.seg", segment_bytes);

// 5. RECORD in metadata
metadata.add_segment(SegmentInfo {
    topic: "orders",
    partition: 0,
    base_offset: 0,
    end_offset: 999,
    s3_path: "data/orders/0/00000000000000000000.seg",
    size_bytes: segment_bytes.len(),
});
```

### Internal Operation

#### Block Building

The writer maintains a current block in memory:

```rust
pub struct SegmentWriter {
    // Current block being built
    current_block: Vec<Record>,
    current_block_size: usize,

    // Completed blocks
    blocks: Vec<Bytes>,

    // Configuration
    block_size_target: usize,  // 1MB default
    compression: Compression,   // LZ4 default
}

impl SegmentWriter {
    pub fn append(&mut self, key: Option<&[u8]>, value: &[u8], ts: u64) -> u64 {
        let offset = self.next_offset;
        self.next_offset += 1;

        // Add to current block
        self.current_block.push(Record { offset, timestamp: ts, key, value });
        self.current_block_size += estimate_size(&record);

        // Flush block if it's full
        if self.current_block_size >= self.block_size_target {
            self.flush_block();
        }

        offset
    }
}
```

#### Block Flushing

When a block reaches ~1MB:

```rust
fn flush_block(&mut self) {
    let block_base_offset = self.current_block[0].offset;
    let block_base_timestamp = self.current_block[0].timestamp;

    // 1. Serialize records with delta encoding
    let mut data = BytesMut::new();
    for record in &self.current_block {
        put_varint(&mut data, record.offset - block_base_offset);  // Delta!
        put_varint(&mut data, record.timestamp - block_base_timestamp);  // Delta!
        put_varint(&mut data, record.key.as_ref().map(|k| k.len()).unwrap_or(-1));
        if let Some(key) = &record.key {
            data.extend_from_slice(key);
        }
        put_varint(&mut data, record.value.len());
        data.extend_from_slice(&record.value);
    }

    // 2. Compress the serialized data
    let uncompressed_size = data.len();
    let compressed = lz4_flex::compress_prepend_size(&data);

    // 3. Build block with header
    let mut block = BytesMut::new();
    block.put_u32(uncompressed_size);
    block.put_u32(compressed.len());
    block.put_u32(self.current_block.len());
    block.put_u32(crc32fast::hash(&compressed));
    block.extend_from_slice(&compressed);

    // 4. Add index entry
    self.index_entries.push(IndexEntry {
        offset: block_base_offset,
        position: current_file_position,
    });

    // 5. Save block and reset
    self.blocks.push(block.freeze());
    self.current_block.clear();
}
```

#### Finishing

When the segment is complete:

```rust
pub fn finish(mut self) -> Bytes {
    // Flush any remaining records
    self.flush_block();

    let mut output = BytesMut::new();

    // Write header
    output.put_slice(b"STRM");
    output.put_u16(1);  // version
    output.put_u16(self.compression as u16);
    // ... all header fields ...

    // Write all blocks
    for block in &self.blocks {
        output.extend_from_slice(block);
    }

    // Write index
    let index_offset = output.len();
    output.put_u32(self.index_entries.len());
    for entry in &self.index_entries {
        output.put_u64(entry.offset);
        output.put_u64(entry.position);
    }

    // Write footer
    output.put_u64(index_offset);
    output.put_u32(index_size);
    output.put_u32(crc32fast::hash(&output));
    output.put_slice(&[0u8; 12]);  // reserved
    output.put_slice(b"MRTS");

    output.freeze()
}
```

---

## Key Design Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| **Compression** | LZ4 | Best speed/ratio balance (500MB/s compress, 80% savings) |
| **Block Size** | 1MB | Good compression ratio, fits in memory, fast decompression |
| **Index Granularity** | Per-block | Simple, fast binary search, minimal overhead |
| **Encoding** | Varints + Deltas | Efficient for sequential data (offsets, timestamps) |
| **Segment Size** | 64-256MB | Balances S3 costs vs data freshness |
| **Magic Bytes** | Yes (STRM/MRTS) | Easy file type detection and corruption identification |
| **CRC Checksums** | Per-block + file | Catch corruption early, fail fast |

---

## S3 Integration

### File Naming Convention

```
s3://{bucket}/data/{topic}/{partition}/{base_offset:020}.seg

Examples:
s3://my-bucket/data/orders/0/00000000000000000000.seg  ← offsets 0-99,999
s3://my-bucket/data/orders/0/00000000000000100000.seg  ← offsets 100,000-199,999
s3://my-bucket/data/orders/1/00000000000000000000.seg  ← partition 1
```

Zero-padded to 20 digits ensures lexicographic sorting matches numeric ordering.

### Upload Process

```rust
pub async fn upload_segment(
    s3: &S3Client,
    bucket: &str,
    topic: &str,
    partition: u32,
    base_offset: u64,
    data: Bytes,
) -> Result<String> {
    let key = format!(
        "data/{}/{}/{:020}.seg",
        topic, partition, base_offset
    );

    // Retry with exponential backoff
    for attempt in 0..3 {
        match s3.put_object()
            .bucket(bucket)
            .key(&key)
            .body(data.clone().into())
            .send()
            .await
        {
            Ok(_) => return Ok(key),
            Err(e) if attempt < 2 => {
                tokio::time::sleep(Duration::from_millis(100 << attempt)).await;
            }
            Err(e) => return Err(e.into()),
        }
    }
    unreachable!()
}
```

---

## Performance Characteristics

### Write Performance

**Target:** 50,000 records/sec on a single partition

**Breakdown:**
- Record append to buffer: ~200ns (in-memory operation)
- Block flush (1MB → 256KB compressed): ~2ms every 1000 records
- S3 upload (256MB segment): ~500ms every 4 minutes

**Bottleneck:** Network upload to S3, not CPU or compression

### Compression Ratios

Typical results with LZ4:

| Data Type | Original Size | Compressed Size | Ratio |
|-----------|--------------|-----------------|-------|
| JSON logs | 100 MB | 15-20 MB | 80-85% |
| Structured events | 100 MB | 20-25 MB | 75-80% |
| Already compressed | 100 MB | 95-98 MB | 2-5% |

### Read Performance

**Cached read (segment in local cache):**
- Offset lookup via index: O(log n) ~1-5μs
- Block decompression: ~5ms for 1MB block
- Record extraction: ~100ns per record
- **Total**: ~5-10ms to first record

**Uncached read (download from S3):**
- S3 GET latency: 50-200ms
- Plus cached read time
- **Total**: ~60-220ms to first record

**Prefetching optimization:** Start downloading next segment while reading current one

---

## Testing Strategy

### Unit Tests

```rust
#[test]
fn test_write_read_roundtrip() {
    let mut writer = SegmentWriter::new("test", 0, 0);

    // Write 1000 records
    for i in 0..1000 {
        writer.append(
            Some(format!("key-{}", i).as_bytes()),
            format!("value-{}", i).as_bytes(),
            1000000 + i,
        );
    }

    let bytes = writer.finish();
    let reader = SegmentReader::open(bytes).unwrap();

    // Read back and verify
    let records = reader.read_from(0, 1000).unwrap();
    assert_eq!(records.len(), 1000);
    assert_eq!(records[0].value, b"value-0");
    assert_eq!(records[999].value, b"value-999");
}

#[test]
fn test_compression_reduces_size() {
    let mut writer = SegmentWriter::new("test", 0, 0);

    // Write repetitive data (compresses well)
    for i in 0..10000 {
        writer.append(None, b"repetitive data", 1000000);
    }

    let bytes = writer.finish();

    // Original: 10,000 * ~15 bytes = 150KB
    // Compressed should be much smaller
    assert!(bytes.len() < 50_000);  // < 33% of original
}

#[test]
fn test_offset_lookup() {
    let mut writer = SegmentWriter::new("test", 0, 0);

    for i in 0..5000 {
        writer.append(None, format!("msg-{}", i).as_bytes(), 1000000 + i);
    }

    let bytes = writer.finish();
    let reader = SegmentReader::open(bytes).unwrap();

    // Should efficiently find record at offset 2500
    let records = reader.read_from(2500, 1).unwrap();
    assert_eq!(records[0].offset, 2500);
    assert_eq!(records[0].value, b"msg-2500");
}
```

### Benchmarks

```rust
#[bench]
fn bench_append_throughput(b: &mut Bencher) {
    b.iter(|| {
        let mut writer = SegmentWriter::new("test", 0, 0);
        for i in 0..10000 {
            writer.append(None, b"test message", 1000000);
        }
    });
}

#[bench]
fn bench_compression_speed(b: &mut Bencher) {
    let mut writer = SegmentWriter::new("test", 0, 0);
    for i in 0..10000 {
        writer.append(None, b"test message data", 1000000);
    }

    b.iter(|| {
        writer.clone().finish()
    });
}
```

---

## Comparison to Kafka

| Aspect | Kafka Log Segment | Our Segment Format |
|--------|-------------------|-------------------|
| Storage | Local disk (ext4/xfs) | S3 (object storage) |
| Typical Size | 1GB default | 64-256MB |
| Index | Sparse (every 4KB of data) | Per-block (~1MB) |
| Compression | Per-batch (producer-side) | Per-block (storage-side) |
| Replication | Broker-to-broker over network | S3 handles it (99.999999999% durability) |
| Format | Custom binary with offsets | Similar, but optimized for S3 |
| CRC | Per-batch | Per-block + full file |

**Key Insight:** We can simplify because S3 already provides replication, durability, and availability. Kafka needs complex broker-to-broker replication; we don't.

---

## Dependencies

```toml
[dependencies]
# Data structures
bytes = "1.0"

# Compression
lz4_flex = "0.11"  # Fast LZ4 compression
# Alternative: zstd = "0.13" for better ratios, slower speed

# Checksums
crc32fast = "1.3"

# Serialization
# (varints implemented manually for max efficiency)
```

---

## Deliverables

At the end of Initiative 1.2, you will have:

- [ ] **SegmentWriter implementation**
  - Accepts records via `append()`
  - Produces compressed segment files via `finish()`
  - Handles block batching and compression automatically

- [ ] **SegmentReader implementation**
  - Opens segment files
  - Reads from arbitrary offsets using index
  - Decompresses blocks on demand

- [ ] **Unit tests**
  - Write/read roundtrip
  - Compression verification
  - Offset lookup correctness
  - Edge cases (empty segments, single record, etc.)

- [ ] **Benchmarks**
  - Write throughput (records/sec)
  - Compression speed and ratio
  - Read latency (cached vs uncached)

- [ ] **Documentation**
  - Format specification
  - API documentation
  - Performance characteristics

---

## Next Steps

After completing the Segment Writer (Initiative 1.2), you'll move to:

**Initiative 1.3: Metadata Store** - Track which segments exist, what offsets they contain, and where they're stored in S3. The metadata store is what allows the system to quickly find "which segment contains offset 15,000?" without scanning all files.

---

## Further Reading

- [LZ4 Compression Algorithm](https://github.com/lz4/lz4)
- [Variable-Length Encoding (Varints)](https://developers.google.com/protocol-buffers/docs/encoding#varints)
- [S3 Performance Best Practices](https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html)
- [Kafka Log Format](https://kafka.apache.org/documentation/#recordbatch) (for comparison)
