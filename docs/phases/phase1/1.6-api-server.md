# Initiative 1.6: API Server

**Timeline:** Week 5-6
**Goal:** Expose produce and consume functionality via gRPC APIs

---

## Overview

The **API Server** provides the network interface for clients to interact with StreamHouse. It exposes gRPC services for producing, consuming, and managing topics.

At this stage (Phase 1), we're building our own protocol. In Phase 2, we'll add Kafka protocol compatibility on top of this foundation.

---

## What We're Building

```
┌──────────────────────────────────────────────────┐
│              StreamHouse Server                  │
├──────────────────────────────────────────────────┤
│                                                  │
│  ┌────────────────────────────────────────────┐ │
│  │           gRPC Services                     │ │
│  │  ┌──────────┐  ┌──────────┐  ┌──────────┐ │ │
│  │  │ Producer │  │ Consumer │  │  Admin   │ │ │
│  │  │ Service  │  │ Service  │  │ Service  │ │ │
│  │  └────┬─────┘  └────┬─────┘  └────┬─────┘ │ │
│  └───────┼─────────────┼─────────────┼───────┘ │
│          │             │             │          │
│          └─────────────┴─────────────┘          │
│                        │                        │
│  ┌─────────────────────▼──────────────────────┐ │
│  │         Storage Layer                      │ │
│  │  (StorageManager, PartitionReader/Writer) │ │
│  └────────────────────────────────────────────┘ │
└──────────────────────────────────────────────────┘
```

---

## Protocol Definition

### Proto File

```protobuf
// crates/streamhouse-server/proto/streamhouse.proto

syntax = "proto3";

package streamhouse.v1;

// =============================================================================
// PRODUCER SERVICE
// =============================================================================

service ProducerService {
    // Produce a single record
    rpc Produce(ProduceRequest) returns (ProduceResponse);

    // Produce multiple records in a stream
    rpc ProduceStream(stream ProduceRequest) returns (stream ProduceResponse);
}

message ProduceRequest {
    string topic = 1;
    optional bytes key = 2;
    bytes value = 3;
    optional int32 partition = 4;  // if not set, auto-assign by key hash
    optional int64 timestamp = 5;  // if not set, use server time
}

message ProduceResponse {
    string topic = 1;
    int32 partition = 2;
    int64 offset = 3;
    int64 timestamp = 4;
}

// =============================================================================
// CONSUMER SERVICE
// =============================================================================

service ConsumerService {
    // Consume records from a partition
    rpc Consume(ConsumeRequest) returns (stream ConsumeResponse);

    // Commit offset for a consumer group
    rpc CommitOffset(CommitOffsetRequest) returns (CommitOffsetResponse);

    // Get committed offset for a consumer group
    rpc GetCommittedOffset(GetCommittedOffsetRequest) returns (GetCommittedOffsetResponse);
}

message ConsumeRequest {
    string topic = 1;
    int32 partition = 2;
    int64 start_offset = 3;  // -1 for earliest, -2 for latest
    int32 max_records = 4;   // per batch
    int32 max_wait_ms = 5;   // long polling timeout
    optional string group_id = 6;
}

message ConsumeResponse {
    repeated Record records = 1;
    int64 high_watermark = 2;
}

message Record {
    int64 offset = 1;
    int64 timestamp = 2;
    optional bytes key = 3;
    bytes value = 4;
}

message CommitOffsetRequest {
    string group_id = 1;
    string topic = 2;
    int32 partition = 3;
    int64 offset = 4;
    optional string metadata = 5;
}

message CommitOffsetResponse {
    // Empty on success, error in status
}

message GetCommittedOffsetRequest {
    string group_id = 1;
    string topic = 2;
    int32 partition = 3;
}

message GetCommittedOffsetResponse {
    optional int64 offset = 1;
}

// =============================================================================
// ADMIN SERVICE
// =============================================================================

service AdminService {
    // Create a topic
    rpc CreateTopic(CreateTopicRequest) returns (CreateTopicResponse);

    // Delete a topic
    rpc DeleteTopic(DeleteTopicRequest) returns (DeleteTopicResponse);

    // List all topics
    rpc ListTopics(ListTopicsRequest) returns (ListTopicsResponse);

    // Get topic metadata
    rpc GetTopicMetadata(GetTopicMetadataRequest) returns (GetTopicMetadataResponse);
}

message CreateTopicRequest {
    string name = 1;
    int32 partition_count = 2;
    optional int64 retention_ms = 3;
    map<string, string> config = 4;
}

message CreateTopicResponse {
    string name = 1;
    int32 partition_count = 2;
}

message DeleteTopicRequest {
    string name = 1;
}

message DeleteTopicResponse {
    // Empty on success
}

message ListTopicsRequest {
    // Empty
}

message ListTopicsResponse {
    repeated TopicInfo topics = 1;
}

message TopicInfo {
    string name = 1;
    int32 partition_count = 2;
    optional int64 retention_ms = 3;
    int64 created_at = 4;
}

message GetTopicMetadataRequest {
    string name = 1;
}

message GetTopicMetadataResponse {
    string name = 1;
    int32 partition_count = 2;
    repeated PartitionInfo partitions = 3;
}

message PartitionInfo {
    int32 partition_id = 1;
    int64 high_watermark = 2;
    int64 earliest_offset = 3;
}
```

---

## Rust Implementation

### Build Script

```rust
// crates/streamhouse-server/build.rs

fn main() -> Result<(), Box<dyn std::error::Error>> {
    tonic_build::configure()
        .build_server(true)
        .build_client(false)
        .compile(
            &["proto/streamhouse.proto"],
            &["proto"],
        )?;
    Ok(())
}
```

### Producer Service

```rust
// crates/streamhouse-server/src/services/producer.rs

use streamhouse_storage::StorageManager;
use tonic::{Request, Response, Status};
use std::sync::Arc;

pub mod pb {
    tonic::include_proto!("streamhouse.v1");
}

pub struct ProducerServiceImpl {
    storage: Arc<StorageManager>,
}

impl ProducerServiceImpl {
    pub fn new(storage: Arc<StorageManager>) -> Self {
        Self { storage }
    }
}

#[tonic::async_trait]
impl pb::producer_service_server::ProducerService for ProducerServiceImpl {
    async fn produce(
        &self,
        request: Request<pb::ProduceRequest>,
    ) -> Result<Response<pb::ProduceResponse>, Status> {
        let req = request.into_inner();

        let key = req.key.map(bytes::Bytes::from);
        let value = bytes::Bytes::from(req.value);
        let timestamp = req.timestamp.map(|t| t as u64);

        let result = self
            .storage
            .append(&req.topic, key, value, timestamp)
            .await
            .map_err(|e| Status::internal(format!("Failed to produce: {}", e)))?;

        Ok(Response::new(pb::ProduceResponse {
            topic: result.topic,
            partition: result.partition as i32,
            offset: result.offset as i64,
            timestamp: result.timestamp as i64,
        }))
    }

    async fn produce_stream(
        &self,
        request: Request<tonic::Streaming<pb::ProduceRequest>>,
    ) -> Result<Response<Self::ProduceStreamStream>, Status> {
        let mut stream = request.into_inner();
        let storage = self.storage.clone();

        let output_stream = async_stream::try_stream! {
            while let Some(req) = stream.message().await? {
                let key = req.key.map(bytes::Bytes::from);
                let value = bytes::Bytes::from(req.value);
                let timestamp = req.timestamp.map(|t| t as u64);

                let result = storage
                    .append(&req.topic, key, value, timestamp)
                    .await
                    .map_err(|e| Status::internal(format!("Failed to produce: {}", e)))?;

                yield pb::ProduceResponse {
                    topic: result.topic,
                    partition: result.partition as i32,
                    offset: result.offset as i64,
                    timestamp: result.timestamp as i64,
                };
            }
        };

        Ok(Response::new(Box::pin(output_stream)))
    }
}
```

### Consumer Service

```rust
// crates/streamhouse-server/src/services/consumer.rs

use streamhouse_storage::{PartitionReader, Consumer};
use streamhouse_metadata::MetadataStore;
use tonic::{Request, Response, Status};
use std::sync::Arc;

pub struct ConsumerServiceImpl {
    readers: Arc<ReaderManager>,
    metadata: Arc<dyn MetadataStore>,
}

impl ConsumerServiceImpl {
    pub fn new(readers: Arc<ReaderManager>, metadata: Arc<dyn MetadataStore>) -> Self {
        Self { readers, metadata }
    }
}

#[tonic::async_trait]
impl pb::consumer_service_server::ConsumerService for ConsumerServiceImpl {
    type ConsumeStream = Pin<Box<dyn Stream<Item = Result<pb::ConsumeResponse, Status>> + Send>>;

    async fn consume(
        &self,
        request: Request<pb::ConsumeRequest>,
    ) -> Result<Response<Self::ConsumeStream>, Status> {
        let req = request.into_inner();

        // Get reader for partition
        let reader = self
            .readers
            .get_reader(&req.topic, req.partition as u32)
            .await
            .map_err(|e| Status::internal(format!("Failed to get reader: {}", e)))?;

        // Create consumer
        let mut consumer = Consumer::new(
            req.topic.clone(),
            req.partition as u32,
            req.group_id,
            reader,
            self.metadata.clone(),
        )
        .await
        .map_err(|e| Status::internal(format!("Failed to create consumer: {}", e)))?;

        // Handle special offsets
        let start_offset = match req.start_offset {
            -1 => 0, // earliest
            -2 => {
                // latest
                let partition = self
                    .metadata
                    .get_partition(&req.topic, req.partition as u32)
                    .await
                    .map_err(|e| Status::internal(e.to_string()))?
                    .ok_or_else(|| Status::not_found("Partition not found"))?;
                partition.high_watermark
            }
            offset if offset >= 0 => offset as u64,
            _ => return Err(Status::invalid_argument("Invalid start_offset")),
        };

        consumer.seek(start_offset);

        let max_records = req.max_records as usize;
        let max_wait_ms = req.max_wait_ms as u64;

        // Stream records
        let output_stream = async_stream::try_stream! {
            loop {
                let records = consumer
                    .poll(max_records)
                    .await
                    .map_err(|e| Status::internal(format!("Failed to poll: {}", e)))?;

                if records.is_empty() {
                    if max_wait_ms > 0 {
                        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
                        continue;
                    } else {
                        break;
                    }
                }

                let pb_records: Vec<pb::Record> = records
                    .into_iter()
                    .map(|r| pb::Record {
                        offset: r.offset as i64,
                        timestamp: r.timestamp as i64,
                        key: r.key.map(|k| k.to_vec()),
                        value: r.value.to_vec(),
                    })
                    .collect();

                let high_watermark = consumer.position() as i64;

                yield pb::ConsumeResponse {
                    records: pb_records,
                    high_watermark,
                };
            }
        };

        Ok(Response::new(Box::pin(output_stream)))
    }

    async fn commit_offset(
        &self,
        request: Request<pb::CommitOffsetRequest>,
    ) -> Result<Response<pb::CommitOffsetResponse>, Status> {
        let req = request.into_inner();

        self.metadata
            .commit_offset(
                &req.group_id,
                &req.topic,
                req.partition as u32,
                req.offset as u64,
                req.metadata,
            )
            .await
            .map_err(|e| Status::internal(format!("Failed to commit offset: {}", e)))?;

        Ok(Response::new(pb::CommitOffsetResponse {}))
    }

    async fn get_committed_offset(
        &self,
        request: Request<pb::GetCommittedOffsetRequest>,
    ) -> Result<Response<pb::GetCommittedOffsetResponse>, Status> {
        let req = request.into_inner();

        let offset = self
            .metadata
            .get_committed_offset(&req.group_id, &req.topic, req.partition as u32)
            .await
            .map_err(|e| Status::internal(format!("Failed to get committed offset: {}", e)))?;

        Ok(Response::new(pb::GetCommittedOffsetResponse {
            offset: offset.map(|o| o as i64),
        }))
    }
}
```

### Admin Service

```rust
// crates/streamhouse-server/src/services/admin.rs

use streamhouse_metadata::{MetadataStore, TopicConfig};
use tonic::{Request, Response, Status};
use std::sync::Arc;
use std::collections::HashMap;

pub struct AdminServiceImpl {
    metadata: Arc<dyn MetadataStore>,
}

impl AdminServiceImpl {
    pub fn new(metadata: Arc<dyn MetadataStore>) -> Self {
        Self { metadata }
    }
}

#[tonic::async_trait]
impl pb::admin_service_server::AdminService for AdminServiceImpl {
    async fn create_topic(
        &self,
        request: Request<pb::CreateTopicRequest>,
    ) -> Result<Response<pb::CreateTopicResponse>, Status> {
        let req = request.into_inner();

        let config = TopicConfig {
            name: req.name.clone(),
            partition_count: req.partition_count as u32,
            retention_ms: req.retention_ms,
            config: req.config,
        };

        self.metadata
            .create_topic(config)
            .await
            .map_err(|e| Status::already_exists(format!("Failed to create topic: {}", e)))?;

        Ok(Response::new(pb::CreateTopicResponse {
            name: req.name,
            partition_count: req.partition_count,
        }))
    }

    async fn delete_topic(
        &self,
        request: Request<pb::DeleteTopicRequest>,
    ) -> Result<Response<pb::DeleteTopicResponse>, Status> {
        let req = request.into_inner();

        self.metadata
            .delete_topic(&req.name)
            .await
            .map_err(|e| Status::internal(format!("Failed to delete topic: {}", e)))?;

        Ok(Response::new(pb::DeleteTopicResponse {}))
    }

    async fn list_topics(
        &self,
        _request: Request<pb::ListTopicsRequest>,
    ) -> Result<Response<pb::ListTopicsResponse>, Status> {
        let topics = self
            .metadata
            .list_topics()
            .await
            .map_err(|e| Status::internal(format!("Failed to list topics: {}", e)))?;

        let topic_infos: Vec<pb::TopicInfo> = topics
            .into_iter()
            .map(|t| pb::TopicInfo {
                name: t.name,
                partition_count: t.partition_count as i32,
                retention_ms: t.retention_ms,
                created_at: t.created_at,
            })
            .collect();

        Ok(Response::new(pb::ListTopicsResponse {
            topics: topic_infos,
        }))
    }

    async fn get_topic_metadata(
        &self,
        request: Request<pb::GetTopicMetadataRequest>,
    ) -> Result<Response<pb::GetTopicMetadataResponse>, Status> {
        let req = request.into_inner();

        let topic = self
            .metadata
            .get_topic(&req.name)
            .await
            .map_err(|e| Status::internal(e.to_string()))?
            .ok_or_else(|| Status::not_found("Topic not found"))?;

        let partitions = self
            .metadata
            .list_partitions(&req.name)
            .await
            .map_err(|e| Status::internal(e.to_string()))?;

        let partition_infos: Vec<pb::PartitionInfo> = partitions
            .into_iter()
            .map(|p| pb::PartitionInfo {
                partition_id: p.partition_id as i32,
                high_watermark: p.high_watermark as i64,
                earliest_offset: 0, // TODO: track earliest offset
            })
            .collect();

        Ok(Response::new(pb::GetTopicMetadataResponse {
            name: topic.name,
            partition_count: topic.partition_count as i32,
            partitions: partition_infos,
        }))
    }
}
```

### Main Server

```rust
// crates/streamhouse-server/src/main.rs

use streamhouse_server::services::{
    producer::ProducerServiceImpl,
    consumer::ConsumerServiceImpl,
    admin::AdminServiceImpl,
};
use streamhouse_storage::StorageManager;
use streamhouse_metadata::SqliteMetadataStore;
use tonic::transport::Server;
use std::sync::Arc;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize tracing
    tracing_subscriber::fmt::init();

    // Load configuration
    let config = load_config()?;

    // Setup metadata store
    let metadata = Arc::new(SqliteMetadataStore::new(&config.metadata_path).await?);

    // Setup object store (S3 or MinIO)
    let object_store = setup_object_store(&config)?;

    // Setup storage manager
    let storage = Arc::new(StorageManager::new(
        object_store,
        metadata.clone(),
        config.write_config,
    ));

    // Setup reader manager
    let readers = Arc::new(ReaderManager::new(/* ... */));

    // Create services
    let producer_service = ProducerServiceImpl::new(storage);
    let consumer_service = ConsumerServiceImpl::new(readers, metadata.clone());
    let admin_service = AdminServiceImpl::new(metadata);

    // Start gRPC server
    let addr = format!("{}:{}", config.host, config.port).parse()?;

    tracing::info!("StreamHouse server starting on {}", addr);

    Server::builder()
        .add_service(pb::producer_service_server::ProducerServiceServer::new(producer_service))
        .add_service(pb::consumer_service_server::ConsumerServiceServer::new(consumer_service))
        .add_service(pb::admin_service_server::AdminServiceServer::new(admin_service))
        .serve(addr)
        .await?;

    Ok(())
}
```

---

## Configuration

```toml
# config.toml

[server]
host = "0.0.0.0"
port = 50051

[metadata]
path = "./data/metadata.db"

[storage]
s3_bucket = "streamhouse"
s3_region = "us-east-1"
s3_endpoint = "http://localhost:9000"  # MinIO for local dev

[storage.write]
segment_max_size = 67108864  # 64MB
segment_max_age_ms = 600000  # 10 minutes

[storage.read]
cache_dir = "/tmp/streamhouse-cache"
cache_max_size = 1073741824  # 1GB
```

---

## Client Usage Example

```rust
// Example client code
use streamhouse::pb::producer_service_client::ProducerServiceClient;
use streamhouse::pb::ProduceRequest;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let mut client = ProducerServiceClient::connect("http://localhost:50051").await?;

    let request = tonic::Request::new(ProduceRequest {
        topic: "orders".to_string(),
        key: Some(b"user-123".to_vec()),
        value: br#"{"order_id": 456, "amount": 99.99}"#.to_vec(),
        partition: None,
        timestamp: None,
    });

    let response = client.produce(request).await?;
    println!("Produced: {:?}", response.into_inner());

    Ok(())
}
```

---

## Testing

```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_produce_and_consume() {
        // Start test server
        let (server, addr) = start_test_server().await;

        // Connect client
        let mut producer_client = ProducerServiceClient::connect(format!("http://{}", addr))
            .await
            .unwrap();

        // Create topic
        let mut admin_client = AdminServiceClient::connect(format!("http://{}", addr))
            .await
            .unwrap();

        admin_client
            .create_topic(CreateTopicRequest {
                name: "test".to_string(),
                partition_count: 1,
                retention_ms: None,
                config: HashMap::new(),
            })
            .await
            .unwrap();

        // Produce
        let response = producer_client
            .produce(ProduceRequest {
                topic: "test".to_string(),
                key: None,
                value: b"test message".to_vec(),
                partition: None,
                timestamp: None,
            })
            .await
            .unwrap();

        let produce_resp = response.into_inner();
        assert_eq!(produce_resp.partition, 0);
        assert_eq!(produce_resp.offset, 0);

        // Consume
        let mut consumer_client = ConsumerServiceClient::connect(format!("http://{}", addr))
            .await
            .unwrap();

        let mut stream = consumer_client
            .consume(ConsumeRequest {
                topic: "test".to_string(),
                partition: 0,
                start_offset: 0,
                max_records: 10,
                max_wait_ms: 1000,
                group_id: None,
            })
            .await
            .unwrap()
            .into_inner();

        let response = stream.message().await.unwrap().unwrap();
        assert_eq!(response.records.len(), 1);
        assert_eq!(response.records[0].value, b"test message");
    }
}
```

---

## Deliverables

At the end of Initiative 1.6, you will have:

- [ ] **gRPC protocol definition** (proto files)
- [ ] **Producer service** implementation
- [ ] **Consumer service** with streaming
- [ ] **Admin service** for topic management
- [ ] **Server binary** that starts and serves requests
- [ ] **Integration tests** verifying end-to-end flow
- [ ] **Client examples** for documentation

---

## Next Steps

After completing the API Server (Initiative 1.6), you'll move to:

**Initiative 1.7: CLI Tool** - Build a command-line interface for easy interaction with the system.

---

## Dependencies

```toml
[dependencies]
streamhouse-storage = { path = "../streamhouse-storage" }
streamhouse-metadata = { path = "../streamhouse-metadata" }
tonic = "0.11"
prost = "0.12"
tokio = { version = "1.35", features = ["full"] }
async-stream = "0.3"
tracing = "0.1"
tracing-subscriber = "0.3"
serde = { version = "1.0", features = ["derive"] }
toml = "0.8"

[build-dependencies]
tonic-build = "0.11"
```

---

## Time Estimate

**Total: 1-1.5 weeks** (assuming 25-35 hours of work)

- Proto definition: 3-4 hours
- Producer service: 4-6 hours
- Consumer service: 5-7 hours
- Admin service: 3-4 hours
- Server setup and config: 3-4 hours
- Testing: 5-7 hours
- Documentation: 2-3 hours
