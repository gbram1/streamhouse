# Initiative 1.8: Testing & Documentation

**Timeline:** Week 6-7
**Goal:** Comprehensive testing, benchmarking, and documentation to finalize Phase 1

---

## Overview

This final initiative ensures Phase 1 is production-quality:
- **Testing**: Verify correctness and catch regressions
- **Benchmarking**: Measure performance and identify bottlenecks
- **Documentation**: Enable others to use and contribute

By the end, you'll have a solid foundation to build Phase 2 on.

---

## Testing Strategy

### Test Pyramid

```
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚ Integration  â”‚  â† 20% (End-to-end flows)
             â”‚    Tests     â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Component     â”‚  â† 30% (Module interactions)
            â”‚    Tests       â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    Unit Tests        â”‚  â† 50% (Individual functions)
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Unit Tests

Test individual functions and components in isolation.

### Segment Writer/Reader Tests

```rust
// crates/streamhouse-storage/tests/segment_tests.rs

#[cfg(test)]
mod segment_tests {
    use streamhouse_storage::segment::{SegmentWriter, SegmentReader};

    #[test]
    fn test_write_read_roundtrip() {
        let mut writer = SegmentWriter::new("test", 0, 0, 1024 * 1024);

        // Write 1000 records
        for i in 0..1000 {
            writer.append(
                Some(format!("key-{}", i).into()),
                format!("value-{}", i).into(),
                1000000 + i,
            );
        }

        let bytes = writer.finish();
        let reader = SegmentReader::open(bytes).unwrap();

        // Verify all records
        let records = reader.read_from(0, 1000).unwrap();
        assert_eq!(records.len(), 1000);

        for (i, record) in records.iter().enumerate() {
            assert_eq!(record.offset, i as u64);
            assert_eq!(record.value, format!("value-{}", i).as_bytes());
        }
    }

    #[test]
    fn test_compression_effectiveness() {
        let mut writer = SegmentWriter::new("test", 0, 0, 1024 * 1024);

        // Write repetitive data (should compress well)
        for _ in 0..10000 {
            writer.append(None, b"repetitive data".to_vec().into(), 1000000);
        }

        let bytes = writer.finish();

        // Original: ~150KB, compressed should be <50KB
        let uncompressed_size = 10000 * 15;
        assert!(
            bytes.len() < uncompressed_size / 3,
            "Compression ratio should be at least 3x"
        );
    }

    #[test]
    fn test_offset_lookup_performance() {
        let mut writer = SegmentWriter::new("test", 0, 0, 1024 * 1024);

        for i in 0..10000 {
            writer.append(None, format!("msg-{}", i).into(), 1000000 + i);
        }

        let bytes = writer.finish();
        let reader = SegmentReader::open(bytes).unwrap();

        // Should quickly find record in middle
        let start = std::time::Instant::now();
        let records = reader.read_from(5000, 1).unwrap();
        let elapsed = start.elapsed();

        assert_eq!(records[0].offset, 5000);
        assert!(elapsed.as_millis() < 10, "Offset lookup should be <10ms");
    }
}
```

### Metadata Store Tests

```rust
// crates/streamhouse-metadata/tests/metadata_tests.rs

#[tokio::test]
async fn test_topic_lifecycle() {
    let store = SqliteMetadataStore::new_in_memory().await.unwrap();

    // Create topic
    store.create_topic(TopicConfig {
        name: "test".into(),
        partition_count: 3,
        retention_ms: Some(86400000),
        config: HashMap::new(),
    }).await.unwrap();

    // Verify topic exists
    let topic = store.get_topic("test").await.unwrap().unwrap();
    assert_eq!(topic.name, "test");
    assert_eq!(topic.partition_count, 3);

    // Verify partitions created
    let partitions = store.list_partitions("test").await.unwrap();
    assert_eq!(partitions.len(), 3);

    // Delete topic
    store.delete_topic("test").await.unwrap();

    // Verify deleted
    assert!(store.get_topic("test").await.unwrap().is_none());
}

#[tokio::test]
async fn test_segment_tracking() {
    let store = SqliteMetadataStore::new_in_memory().await.unwrap();

    // Setup
    store.create_topic(TopicConfig {
        name: "test".into(),
        partition_count: 1,
        retention_ms: None,
        config: HashMap::new(),
    }).await.unwrap();

    // Add segments
    for i in 0..5 {
        store.add_segment(SegmentInfo {
            id: format!("seg-{}", i),
            topic: "test".into(),
            partition_id: 0,
            base_offset: i * 1000,
            end_offset: (i + 1) * 1000 - 1,
            record_count: 1000,
            size_bytes: 1024 * 1024,
            s3_bucket: "test".into(),
            s3_key: format!("seg-{}.seg", i),
            created_at: 0,
        }).await.unwrap();
    }

    // Find segment by offset
    let seg = store.find_segment_for_offset("test", 0, 2500).await.unwrap().unwrap();
    assert_eq!(seg.id, "seg-2");
    assert_eq!(seg.base_offset, 2000);
    assert_eq!(seg.end_offset, 2999);

    // List all segments
    let segments = store.get_segments("test", 0).await.unwrap();
    assert_eq!(segments.len(), 5);
}
```

---

## Integration Tests

Test complete workflows end-to-end.

### Write and Read Flow

```rust
// tests/integration_test.rs

use streamhouse_storage::{StorageManager, PartitionReader, WriteConfig};
use streamhouse_metadata::SqliteMetadataStore;
use object_store::memory::InMemory;

#[tokio::test]
async fn test_write_read_flow() {
    // Setup
    let metadata = Arc::new(SqliteMetadataStore::new_in_memory().await.unwrap());
    let object_store = Arc::new(InMemory::new());
    let cache = Arc::new(SegmentCache::new("/tmp/test-cache", 1024 * 1024 * 1024).unwrap());

    let storage = StorageManager::new(
        object_store.clone(),
        metadata.clone(),
        WriteConfig::default(),
    );

    // Create topic
    metadata.create_topic(TopicConfig {
        name: "test".into(),
        partition_count: 1,
        retention_ms: None,
        config: HashMap::new(),
    }).await.unwrap();

    // Write records
    for i in 0..1000 {
        storage.append(
            "test",
            Some(format!("key-{}", i).into()),
            format!("value-{}", i).into(),
            None,
        ).await.unwrap();
    }

    // Flush to ensure data is in S3
    storage.flush_all().await.unwrap();

    // Read records
    let reader = PartitionReader::new(
        "test".into(),
        0,
        metadata.clone(),
        object_store,
        cache,
    );

    let result = reader.read(0, 1000).await.unwrap();
    assert_eq!(result.records.len(), 1000);

    // Verify content
    for (i, record) in result.records.iter().enumerate() {
        assert_eq!(record.offset, i as u64);
        assert_eq!(record.value, format!("value-{}", i).as_bytes());
    }
}
```

### gRPC API Tests

```rust
// crates/streamhouse-server/tests/api_tests.rs

#[tokio::test]
async fn test_produce_consume_api() {
    // Start test server
    let (addr, _handle) = start_test_server().await;

    // Connect clients
    let mut producer = ProducerServiceClient::connect(format!("http://{}", addr))
        .await
        .unwrap();

    let mut admin = AdminServiceClient::connect(format!("http://{}", addr))
        .await
        .unwrap();

    let mut consumer = ConsumerServiceClient::connect(format!("http://{}", addr))
        .await
        .unwrap();

    // Create topic
    admin.create_topic(CreateTopicRequest {
        name: "test".into(),
        partition_count: 1,
        retention_ms: None,
        config: HashMap::new(),
    }).await.unwrap();

    // Produce records
    for i in 0..100 {
        let response = producer.produce(ProduceRequest {
            topic: "test".into(),
            key: None,
            value: format!("message-{}", i).into_bytes(),
            partition: None,
            timestamp: None,
        }).await.unwrap();

        let result = response.into_inner();
        assert_eq!(result.offset, i as i64);
    }

    // Consume records
    let mut stream = consumer.consume(ConsumeRequest {
        topic: "test".into(),
        partition: 0,
        start_offset: 0,
        max_records: 100,
        max_wait_ms: 1000,
        group_id: None,
    }).await.unwrap().into_inner();

    let response = stream.message().await.unwrap().unwrap();
    assert_eq!(response.records.len(), 100);
    assert_eq!(response.records[0].value, b"message-0");
    assert_eq!(response.records[99].value, b"message-99");
}
```

### CLI Tests

```bash
#!/bin/bash
# tests/cli_test.sh

set -e

SERVER="http://localhost:50051"
CLI="../target/debug/streamctl"

echo "Starting CLI integration tests..."

# Test topic creation
echo "Testing topic creation..."
$CLI -s $SERVER topic create cli-test --partitions 2
TOPICS=$($CLI -s $SERVER topic list | grep cli-test | wc -l)
[ "$TOPICS" -eq "1" ] || (echo "Topic creation failed" && exit 1)

# Test produce
echo "Testing produce..."
for i in {1..10}; do
    echo "{\"id\": $i}" | $CLI -s $SERVER produce cli-test
done

# Test consume
echo "Testing consume..."
RECORDS=$($CLI -s $SERVER consume cli-test -b | wc -l)
[ "$RECORDS" -eq "11" ] || (echo "Consume failed: expected 11 lines, got $RECORDS" && exit 1)

# Test topic deletion
echo "Testing topic deletion..."
$CLI -s $SERVER topic delete cli-test --yes
TOPICS=$($CLI -s $SERVER topic list | grep cli-test | wc -l)
[ "$TOPICS" -eq "0" ] || (echo "Topic deletion failed" && exit 1)

echo "âœ… All CLI tests passed"
```

---

## Benchmarks

Measure performance and identify bottlenecks.

### Criterion Benchmarks

```rust
// benches/segment_bench.rs

use criterion::{black_box, criterion_group, criterion_main, Criterion, Throughput};
use streamhouse_storage::segment::{SegmentWriter, SegmentReader};

fn bench_segment_write(c: &mut Criterion) {
    let mut group = c.benchmark_group("segment-write");
    group.throughput(Throughput::Elements(10000));

    group.bench_function("write-10k-records", |b| {
        b.iter(|| {
            let mut writer = SegmentWriter::new("test", 0, 0, 1024 * 1024);

            for i in 0..10000 {
                writer.append(
                    None,
                    black_box(format!("message-{}", i).into()),
                    black_box(1000000),
                );
            }

            writer.finish()
        });
    });

    group.finish();
}

fn bench_segment_read(c: &mut Criterion) {
    // Prepare segment
    let mut writer = SegmentWriter::new("test", 0, 0, 1024 * 1024);
    for i in 0..10000 {
        writer.append(None, format!("message-{}", i).into(), 1000000);
    }
    let bytes = writer.finish();

    let mut group = c.benchmark_group("segment-read");
    group.throughput(Throughput::Elements(100));

    group.bench_function("read-100-records", |b| {
        b.iter(|| {
            let reader = SegmentReader::open(bytes.clone()).unwrap();
            black_box(reader.read_from(0, 100).unwrap())
        });
    });

    group.finish();
}

criterion_group!(benches, bench_segment_write, bench_segment_read);
criterion_main!(benches);
```

### Performance Test Script

```bash
#!/bin/bash
# tests/perf_test.sh

set -e

echo "Performance Testing StreamHouse Phase 1"
echo "========================================"
echo

# Start server
cargo build --release
./target/release/streamhouse-server &
SERVER_PID=$!
sleep 2

# Create test topic
./target/release/streamctl topic create perf-test --partitions 1

echo "Test 1: Write throughput (10K records)"
echo "---------------------------------------"
START=$(date +%s%N)
for i in {1..10000}; do
    echo '{"id": '$i'}' | ./target/release/streamctl produce perf-test
done
END=$(date +%s%N)
ELAPSED=$(( (END - START) / 1000000 ))
THROUGHPUT=$(( 10000 * 1000 / ELAPSED ))
echo "Elapsed: ${ELAPSED}ms"
echo "Throughput: ${THROUGHPUT} records/sec"
echo

echo "Test 2: Read latency (cached)"
echo "-----------------------------"
START=$(date +%s%N)
./target/release/streamctl consume perf-test -b > /dev/null
END=$(date +%s%N)
ELAPSED=$(( (END - START) / 1000000 ))
echo "Elapsed: ${ELAPSED}ms"
echo "Latency per record: $(( ELAPSED / 10000 ))ms"
echo

# Cleanup
kill $SERVER_PID
./target/release/streamctl topic delete perf-test --yes

echo "Performance tests complete"
```

---

## Documentation

### README Update

```markdown
# StreamHouse

A unified S3-native streaming platform that replaces Kafka + Flink.

## Phase 1 Complete âœ…

StreamHouse Phase 1 provides:
- âœ… S3-native append-only log storage
- âœ… Compressed segments with LZ4
- âœ… SQLite metadata tracking
- âœ… gRPC API for produce/consume
- âœ… CLI tool for management

## Quick Start

### Prerequisites

- Rust 1.75+
- Docker & Docker Compose (for MinIO)

### Installation

```bash
# Clone repository
git clone https://github.com/yourusername/streamhouse
cd streamhouse

# Start MinIO (local S3)
./scripts/dev-env.sh

# Build
cargo build --release

# Start server
./target/release/streamhouse-server

# In another terminal, create a topic
./target/release/streamctl topic create orders --partitions 3
```

### Usage

```bash
# Produce
echo '{"order_id": 123, "amount": 99.99}' | streamctl produce orders

# Consume
streamctl consume orders --from-beginning

# List topics
streamctl topic list
```

## Performance

Phase 1 achieves:
- **Write**: 50,000 records/sec (single partition)
- **Read (cached)**: <10ms p99 latency
- **Read (uncached)**: <200ms p99 latency
- **Compression**: 80%+ space savings

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        StreamHouse Server           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  gRPC API                           â”‚
â”‚  â”œâ”€â”€ ProducerService                â”‚
â”‚  â”œâ”€â”€ ConsumerService                â”‚
â”‚  â””â”€â”€ AdminService                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Storage Layer                      â”‚
â”‚  â”œâ”€â”€ SegmentWriter (compression)   â”‚
â”‚  â”œâ”€â”€ SegmentReader (caching)       â”‚
â”‚  â””â”€â”€ S3 Integration                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Metadata Store (SQLite)            â”‚
â”‚  â”œâ”€â”€ Topics & Partitions            â”‚
â”‚  â”œâ”€â”€ Segment Index                  â”‚
â”‚  â””â”€â”€ Consumer Offsets               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
         S3 / MinIO
```

## Documentation

- [Phase 1 Overview](docs/phases/phase1/)
- [Segment Format](docs/phases/phase1/1.2-segment-writer.md)
- [API Reference](docs/api/)
- [CLI Guide](docs/cli.md)

## Roadmap

- [x] Phase 1: Core Storage Layer (Months 1-2)
- [ ] Phase 2: Kafka Protocol (Months 3-4)
- [ ] Phase 3: SQL Processing (Months 5-7)
- [ ] Phase 4: Distributed (Months 8-10)

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md)

## License

MIT OR Apache-2.0
```

### API Documentation

Create OpenAPI/gRPC documentation:

```bash
# Generate protobuf documentation
cargo install protoc-gen-doc
protoc --doc_out=./docs/api --doc_opt=html,api.html proto/*.proto
```

### Architecture Documentation

```markdown
# Architecture Overview

## Storage Format

### Segment Structure

Each segment is a self-contained file stored in S3:

- **Header**: Metadata about the segment
- **Blocks**: Compressed batches of records
- **Index**: Offset â†’ byte position mapping
- **Footer**: Validation checksums

See [Segment Format](phases/phase1/1.2-segment-writer.md) for details.

## Write Path

1. Producer sends record via gRPC
2. StorageManager routes to partition writer
3. PartitionWriter buffers in memory
4. When buffer full, flush to segment
5. When segment full, upload to S3
6. Update metadata store

## Read Path

1. Consumer requests offset via gRPC
2. Query metadata for segment location
3. Check local cache
4. Download from S3 if needed
5. Decompress and return records
6. Prefetch next segment

## Metadata Store

SQLite database tracking:
- Topics and partitions
- Segment locations and offsets
- Consumer group offsets
```

---

## Test Coverage

### Setup Coverage Tool

```bash
# Install tarpaulin
cargo install cargo-tarpaulin

# Run coverage
cargo tarpaulin --out Html --output-dir ./coverage
```

### Coverage Targets

| Component | Target Coverage |
|-----------|----------------|
| Storage layer | 80%+ |
| Metadata store | 90%+ |
| API services | 70%+ |
| CLI | 60%+ |
| Overall | 75%+ |

---

## CI/CD Enhancements

Update `.github/workflows/ci.yml`:

```yaml
# Add coverage job
coverage:
  name: Code Coverage
  runs-on: ubuntu-latest
  steps:
    - uses: actions/checkout@v4
    - uses: dtolnay/rust-toolchain@stable
    - uses: Swatinem/rust-cache@v2

    - name: Install tarpaulin
      run: cargo install cargo-tarpaulin

    - name: Run coverage
      run: cargo tarpaulin --out Xml

    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        files: ./cobertura.xml

# Add integration tests
integration:
  name: Integration Tests
  runs-on: ubuntu-latest
  services:
    minio:
      image: minio/minio
      ports:
        - 9000:9000
  steps:
    - uses: actions/checkout@v4
    - uses: dtolnay/rust-toolchain@stable
    - uses: Swatinem/rust-cache@v2

    - name: Run integration tests
      run: cargo test --test '*' -- --test-threads=1
```

---

## Deliverables

At the end of Initiative 1.8, you will have:

- [ ] **Unit tests** for all components (75%+ coverage)
- [ ] **Integration tests** for end-to-end flows
- [ ] **CLI integration tests** (bash scripts)
- [ ] **Benchmarks** measuring performance
- [ ] **Updated README** with usage examples
- [ ] **API documentation** (generated from protos)
- [ ] **Architecture docs** explaining design
- [ ] **CI/CD** with coverage reporting
- [ ] **Blog post** announcing Phase 1 completion

---

## Success Criteria

Phase 1 is complete when:

âœ… All tests pass
âœ… Code coverage >75%
âœ… Performance targets met:
  - Write: 50K records/sec
  - Read (cached): <10ms p99
  - Compression: 80%+ savings
âœ… Documentation complete
âœ… Can demo end-to-end:
  - Create topic
  - Produce 100K records
  - Consume from beginning
  - Verify data in S3
âœ… Blog post published

---

## Blog Post Outline

```markdown
# Building an S3-Native Streaming Platform in Rust (Phase 1)

## Why S3?

Kafka stores data on local disks, requiring complex replication.
S3 is already durable, replicated, and cheap.

## What We Built

In 8 weeks, we built:
- Custom segment format with LZ4 compression (80%+ savings)
- S3-native storage layer
- gRPC API for produce/consume
- CLI tool for easy interaction

## Performance

Early results:
- 50K records/sec write throughput
- <10ms p99 read latency (cached)
- 99% cost reduction vs Kafka

## Technical Deep Dive

[Link to segment format doc]
[Link to architecture doc]

## What's Next

Phase 2: Kafka protocol compatibility
Phase 3: SQL stream processing

Try it: https://github.com/yourusername/streamhouse
```

---

## Time Estimate

**Total: 1-1.5 weeks** (assuming 25-35 hours of work)

- Unit tests: 6-8 hours
- Integration tests: 5-7 hours
- Benchmarks: 3-4 hours
- Documentation: 6-8 hours
- CI/CD setup: 2-3 hours
- Blog post: 3-4 hours

---

## Phase 1 Complete! ğŸ‰

You now have:
- A working S3-native streaming platform
- Solid test coverage
- Good documentation
- Performance benchmarks
- A foundation for Phase 2

**Next:** Phase 2 - Kafka Protocol Compatibility
