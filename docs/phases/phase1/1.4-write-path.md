# Initiative 1.4: Write Path

**Timeline:** Week 3-4
**Goal:** Implement the complete flow from receiving records to storing them durably in S3

---

## Overview

The **Write Path** is how records flow from producers into durable storage. It orchestrates batching, segment writing, S3 uploads, and metadata updates to ensure every record is safely stored and trackable.

This is where the Segment Writer (1.2) and Metadata Store (1.3) come together into a cohesive system.

---

## The Complete Write Flow

```
Producer                                                          S3
   │                                                              │
   │  1. produce("orders", key="user-123", value={...})          │
   ▼                                                              │
┌──────────────┐                                                  │
│  Ingest API  │  2. Validate topic exists                       │
└──────┬───────┘                                                  │
       │                                                          │
       │  3. Determine partition (hash key or round-robin)       │
       ▼                                                          │
┌──────────────┐                                                  │
│  Partition   │  4. Append to in-memory buffer                  │
│  Writer      │     (SegmentWriter)                             │
└──────┬───────┘                                                  │
       │                                                          │
       │  5. Buffer reaches threshold?                           │
       ▼         YES                                              │
┌──────────────┐                                                  │
│ Flush Block  │  6. Compress records into block                 │
│              │     (every ~1MB)                                 │
└──────┬───────┘                                                  │
       │                                                          │
       │  7. Segment full? (64MB default)                        │
       ▼         YES                                              │
┌──────────────┐                                                  │
│ Roll Segment │  8. Finish current segment                      │
│              │     (add index, footer)                         │
└──────┬───────┘                                                  │
       │                                                          │
       │  9. Upload to S3 ─────────────────────────────────────▶ │
       ▼                                                          │
┌──────────────┐                                                  │
│ Update       │  10. Record segment in metadata                 │
│ Metadata     │      Update high watermark                      │
└──────┬───────┘                                                  │
       │                                                          │
       │  11. Return offset to producer                          │
       ▼                                                          │
   Producer ◀── offset=12345                                      │
```

---

## Components

### 1. Partition Writer

Manages writes to a single partition.

```rust
// crates/streamhouse-storage/src/writer.rs

use bytes::Bytes;
use std::sync::Arc;
use tokio::sync::Mutex;
use streamhouse_metadata::{MetadataStore, SegmentInfo};
use object_store::ObjectStore;

pub struct PartitionWriter {
    topic: String,
    partition_id: u32,

    // Current segment being written
    current_segment: SegmentWriter,
    current_base_offset: u64,

    // S3 and metadata
    object_store: Arc<dyn ObjectStore>,
    metadata: Arc<dyn MetadataStore>,

    // Configuration
    config: WriteConfig,
}

pub struct WriteConfig {
    /// Maximum segment size before rolling (default: 64MB)
    pub segment_max_size: usize,

    /// Maximum time before rolling segment (default: 10 minutes)
    pub segment_max_age_ms: u64,

    /// S3 bucket name
    pub s3_bucket: String,

    /// Block size target for compression (default: 1MB)
    pub block_size_target: usize,

    /// Buffer flush interval (default: 100ms)
    pub buffer_flush_ms: u64,

    /// Number of retries for S3 upload (default: 3)
    pub s3_upload_retries: u32,
}

impl Default for WriteConfig {
    fn default() -> Self {
        Self {
            segment_max_size: 64 * 1024 * 1024, // 64MB
            segment_max_age_ms: 10 * 60 * 1000, // 10 minutes
            s3_bucket: "streamhouse".to_string(),
            block_size_target: 1024 * 1024, // 1MB
            buffer_flush_ms: 100,
            s3_upload_retries: 3,
        }
    }
}

impl PartitionWriter {
    pub async fn new(
        topic: String,
        partition_id: u32,
        object_store: Arc<dyn ObjectStore>,
        metadata: Arc<dyn MetadataStore>,
        config: WriteConfig,
    ) -> Result<Self> {
        // Get current high watermark from metadata
        let partition = metadata
            .get_partition(&topic, partition_id)
            .await?
            .ok_or_else(|| Error::PartitionNotFound(topic.clone(), partition_id))?;

        let current_base_offset = partition.high_watermark;

        let current_segment = SegmentWriter::new(
            topic.clone(),
            partition_id,
            current_base_offset,
            config.block_size_target,
        );

        Ok(Self {
            topic,
            partition_id,
            current_segment,
            current_base_offset,
            object_store,
            metadata,
            config,
        })
    }

    /// Append a record and return its offset
    pub async fn append(
        &mut self,
        key: Option<Bytes>,
        value: Bytes,
        timestamp: u64,
    ) -> Result<u64> {
        // Append to current segment
        let offset = self.current_segment.append(key, value, timestamp);

        // Check if we should roll the segment
        if self.should_roll_segment() {
            self.roll_segment().await?;
        }

        Ok(offset)
    }

    /// Check if we should create a new segment
    fn should_roll_segment(&self) -> bool {
        // Check size threshold
        if self.current_segment.estimated_size() >= self.config.segment_max_size {
            return true;
        }

        // Check age threshold
        if self.current_segment.age_ms() >= self.config.segment_max_age_ms {
            return true;
        }

        false
    }

    /// Finish current segment and start a new one
    async fn roll_segment(&mut self) -> Result<()> {
        // Take ownership of current segment
        let segment = std::mem::replace(
            &mut self.current_segment,
            SegmentWriter::new(
                self.topic.clone(),
                self.partition_id,
                0, // Will be updated below
                self.config.block_size_target,
            ),
        );

        let base_offset = segment.base_offset();
        let end_offset = segment.next_offset() - 1;
        let record_count = segment.record_count();

        // Finish the segment (compress, add index, footer)
        let segment_bytes = segment.finish();
        let size_bytes = segment_bytes.len() as u64;

        // Generate S3 path
        let s3_key = format!(
            "data/{}/{}/{:020}.seg",
            self.topic, self.partition_id, base_offset
        );

        // Upload to S3 with retries
        self.upload_to_s3(&s3_key, segment_bytes.clone()).await?;

        // Record segment in metadata
        let segment_info = SegmentInfo {
            id: format!("{}-{}-{}", self.topic, self.partition_id, base_offset),
            topic: self.topic.clone(),
            partition_id: self.partition_id,
            base_offset,
            end_offset,
            record_count,
            size_bytes,
            s3_bucket: self.config.s3_bucket.clone(),
            s3_key: s3_key.clone(),
            created_at: now_ms(),
        };

        self.metadata.add_segment(segment_info).await?;

        // Update high watermark
        self.metadata
            .update_high_watermark(&self.topic, self.partition_id, end_offset + 1)
            .await?;

        // Start new segment
        self.current_base_offset = end_offset + 1;
        self.current_segment = SegmentWriter::new(
            self.topic.clone(),
            self.partition_id,
            self.current_base_offset,
            self.config.block_size_target,
        );

        tracing::info!(
            topic = %self.topic,
            partition = self.partition_id,
            base_offset,
            end_offset,
            size_bytes,
            s3_key = %s3_key,
            "Segment rolled and uploaded to S3"
        );

        Ok(())
    }

    /// Upload segment to S3 with exponential backoff retry
    async fn upload_to_s3(&self, key: &str, data: Bytes) -> Result<()> {
        let path = object_store::path::Path::from(key);

        for attempt in 0..self.config.s3_upload_retries {
            match self.object_store.put(&path, data.clone()).await {
                Ok(_) => {
                    tracing::debug!(
                        key = %key,
                        size = data.len(),
                        attempt = attempt + 1,
                        "Successfully uploaded segment to S3"
                    );
                    return Ok(());
                }
                Err(e) if attempt < self.config.s3_upload_retries - 1 => {
                    let backoff_ms = 100 * 2_u64.pow(attempt);
                    tracing::warn!(
                        key = %key,
                        attempt = attempt + 1,
                        backoff_ms,
                        error = %e,
                        "S3 upload failed, retrying"
                    );
                    tokio::time::sleep(tokio::time::Duration::from_millis(backoff_ms)).await;
                }
                Err(e) => {
                    tracing::error!(
                        key = %key,
                        error = %e,
                        "S3 upload failed after all retries"
                    );
                    return Err(Error::S3UploadFailed(e.to_string()));
                }
            }
        }

        unreachable!()
    }

    /// Flush any buffered data (called on shutdown)
    pub async fn flush(&mut self) -> Result<()> {
        if self.current_segment.record_count() > 0 {
            self.roll_segment().await?;
        }
        Ok(())
    }
}
```

### 2. Topic Writer

Manages writes across multiple partitions for a single topic.

```rust
// crates/streamhouse-storage/src/writer.rs (continued)

pub struct TopicWriter {
    topic: String,
    partitions: Vec<Arc<Mutex<PartitionWriter>>>,
}

impl TopicWriter {
    pub async fn new(
        topic: String,
        partition_count: u32,
        object_store: Arc<dyn ObjectStore>,
        metadata: Arc<dyn MetadataStore>,
        config: WriteConfig,
    ) -> Result<Self> {
        let mut partitions = Vec::new();

        for partition_id in 0..partition_count {
            let writer = PartitionWriter::new(
                topic.clone(),
                partition_id,
                object_store.clone(),
                metadata.clone(),
                config.clone(),
            )
            .await?;

            partitions.push(Arc::new(Mutex::new(writer)));
        }

        Ok(Self { topic, partitions })
    }

    /// Append a record, automatically selecting partition
    pub async fn append(
        &self,
        key: Option<Bytes>,
        value: Bytes,
        timestamp: Option<u64>,
    ) -> Result<(u32, u64)> {
        // Determine partition
        let partition_id = self.select_partition(&key);

        // Append to partition
        let timestamp = timestamp.unwrap_or_else(now_ms);
        let mut writer = self.partitions[partition_id as usize].lock().await;
        let offset = writer.append(key, value, timestamp).await?;

        Ok((partition_id, offset))
    }

    /// Select partition based on key (or round-robin if no key)
    fn select_partition(&self, key: &Option<Bytes>) -> u32 {
        match key {
            Some(k) => {
                // Hash the key to determine partition
                use std::hash::{Hash, Hasher};
                let mut hasher = std::collections::hash_map::DefaultHasher::new();
                k.hash(&mut hasher);
                let hash = hasher.finish();
                (hash % self.partitions.len() as u64) as u32
            }
            None => {
                // Round-robin (simple, but could use atomic counter)
                use std::sync::atomic::{AtomicU32, Ordering};
                static COUNTER: AtomicU32 = AtomicU32::new(0);
                let partition = COUNTER.fetch_add(1, Ordering::Relaxed);
                partition % self.partitions.len() as u32
            }
        }
    }

    /// Flush all partitions
    pub async fn flush(&self) -> Result<()> {
        for partition in &self.partitions {
            let mut writer = partition.lock().await;
            writer.flush().await?;
        }
        Ok(())
    }
}
```

### 3. Storage Manager

Manages all topic writers.

```rust
// crates/streamhouse-storage/src/manager.rs

use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;

pub struct StorageManager {
    topic_writers: Arc<RwLock<HashMap<String, Arc<TopicWriter>>>>,
    object_store: Arc<dyn ObjectStore>,
    metadata: Arc<dyn MetadataStore>,
    config: WriteConfig,
}

impl StorageManager {
    pub fn new(
        object_store: Arc<dyn ObjectStore>,
        metadata: Arc<dyn MetadataStore>,
        config: WriteConfig,
    ) -> Self {
        Self {
            topic_writers: Arc::new(RwLock::new(HashMap::new())),
            object_store,
            metadata,
            config,
        }
    }

    /// Get or create a topic writer
    async fn get_or_create_writer(&self, topic: &str) -> Result<Arc<TopicWriter>> {
        // Fast path: read lock
        {
            let writers = self.topic_writers.read().await;
            if let Some(writer) = writers.get(topic) {
                return Ok(writer.clone());
            }
        }

        // Slow path: write lock
        let mut writers = self.topic_writers.write().await;

        // Double-check (another thread may have created it)
        if let Some(writer) = writers.get(topic) {
            return Ok(writer.clone());
        }

        // Get topic metadata
        let topic_meta = self
            .metadata
            .get_topic(topic)
            .await?
            .ok_or_else(|| Error::TopicNotFound(topic.to_string()))?;

        // Create writer
        let writer = TopicWriter::new(
            topic.to_string(),
            topic_meta.partition_count,
            self.object_store.clone(),
            self.metadata.clone(),
            self.config.clone(),
        )
        .await?;

        let writer = Arc::new(writer);
        writers.insert(topic.to_string(), writer.clone());

        Ok(writer)
    }

    /// Append a record
    pub async fn append(
        &self,
        topic: &str,
        key: Option<Bytes>,
        value: Bytes,
        timestamp: Option<u64>,
    ) -> Result<AppendResult> {
        let writer = self.get_or_create_writer(topic).await?;
        let (partition, offset) = writer.append(key, value, timestamp).await?;

        Ok(AppendResult {
            topic: topic.to_string(),
            partition,
            offset,
            timestamp: timestamp.unwrap_or_else(now_ms),
        })
    }

    /// Flush all writers
    pub async fn flush_all(&self) -> Result<()> {
        let writers = self.topic_writers.read().await;
        for writer in writers.values() {
            writer.flush().await?;
        }
        Ok(())
    }
}

#[derive(Debug, Clone)]
pub struct AppendResult {
    pub topic: String,
    pub partition: u32,
    pub offset: u64,
    pub timestamp: u64,
}
```

---

## Configuration

```rust
// crates/streamhouse-storage/src/config.rs

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WriteConfig {
    /// Maximum segment size (default: 64MB)
    #[serde(default = "default_segment_max_size")]
    pub segment_max_size: usize,

    /// Maximum segment age in milliseconds (default: 10 minutes)
    #[serde(default = "default_segment_max_age_ms")]
    pub segment_max_age_ms: u64,

    /// S3 bucket name
    pub s3_bucket: String,

    /// S3 region (or endpoint for MinIO)
    pub s3_region: String,

    /// S3 endpoint (optional, for MinIO/custom S3)
    pub s3_endpoint: Option<String>,

    /// Block size for compression (default: 1MB)
    #[serde(default = "default_block_size")]
    pub block_size_target: usize,

    /// Number of S3 upload retries (default: 3)
    #[serde(default = "default_retries")]
    pub s3_upload_retries: u32,
}

fn default_segment_max_size() -> usize {
    64 * 1024 * 1024
}
fn default_segment_max_age_ms() -> u64 {
    10 * 60 * 1000
}
fn default_block_size() -> usize {
    1024 * 1024
}
fn default_retries() -> u32 {
    3
}
```

---

## Usage Example

```rust
use streamhouse_storage::{StorageManager, WriteConfig};
use streamhouse_metadata::SqliteMetadataStore;
use object_store::aws::AmazonS3Builder;

#[tokio::main]
async fn main() -> Result<()> {
    // Setup metadata store
    let metadata = Arc::new(SqliteMetadataStore::new("metadata.db").await?);

    // Setup S3 client (or MinIO for dev)
    let object_store = Arc::new(
        AmazonS3Builder::new()
            .with_bucket_name("streamhouse")
            .with_region("us-east-1")
            .with_endpoint("http://localhost:9000") // MinIO
            .with_access_key_id("minioadmin")
            .with_secret_access_key("minioadmin")
            .build()?,
    );

    // Create storage manager
    let config = WriteConfig {
        s3_bucket: "streamhouse".to_string(),
        s3_region: "us-east-1".to_string(),
        s3_endpoint: Some("http://localhost:9000".to_string()),
        ..Default::default()
    };

    let storage = StorageManager::new(object_store, metadata.clone(), config);

    // Create topic first
    metadata
        .create_topic(TopicConfig {
            name: "orders".to_string(),
            partition_count: 3,
            retention_ms: None,
            config: HashMap::new(),
        })
        .await?;

    // Append records
    for i in 0..10000 {
        let key = format!("user-{}", i % 100);
        let value = format!(r#"{{"order_id": {}, "amount": 99.99}}"#, i);

        let result = storage
            .append(
                "orders",
                Some(Bytes::from(key)),
                Bytes::from(value),
                None, // auto timestamp
            )
            .await?;

        println!(
            "Appended: partition={}, offset={}",
            result.partition, result.offset
        );
    }

    // Flush (ensure all data is written)
    storage.flush_all().await?;

    Ok(())
}
```

---

## Error Handling

```rust
#[derive(Debug, thiserror::Error)]
pub enum Error {
    #[error("Topic not found: {0}")]
    TopicNotFound(String),

    #[error("Partition not found: {0}/{1}")]
    PartitionNotFound(String, u32),

    #[error("S3 upload failed: {0}")]
    S3UploadFailed(String),

    #[error("Metadata error: {0}")]
    MetadataError(#[from] streamhouse_metadata::MetadataError),

    #[error("Object store error: {0}")]
    ObjectStoreError(#[from] object_store::Error),

    #[error("Segment write error: {0}")]
    SegmentError(String),
}

pub type Result<T> = std::result::Result<T, Error>;
```

---

## Testing

### Integration Test

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;

    async fn setup_test_env() -> (StorageManager, TempDir) {
        let temp_dir = TempDir::new().unwrap();
        let db_path = temp_dir.path().join("metadata.db");

        let metadata = Arc::new(SqliteMetadataStore::new(db_path).await.unwrap());

        // Use in-memory object store for testing
        let object_store = Arc::new(object_store::memory::InMemory::new());

        let config = WriteConfig {
            s3_bucket: "test".to_string(),
            s3_region: "us-east-1".to_string(),
            s3_endpoint: None,
            segment_max_size: 1024 * 1024, // 1MB for faster tests
            ..Default::default()
        };

        let storage = StorageManager::new(object_store, metadata.clone(), config);

        // Create test topic
        metadata
            .create_topic(TopicConfig {
                name: "test".to_string(),
                partition_count: 3,
                retention_ms: None,
                config: HashMap::new(),
            })
            .await
            .unwrap();

        (storage, temp_dir)
    }

    #[tokio::test]
    async fn test_append_records() {
        let (storage, _temp) = setup_test_env().await;

        // Append 100 records
        for i in 0..100 {
            let result = storage
                .append(
                    "test",
                    Some(Bytes::from(format!("key-{}", i))),
                    Bytes::from(format!("value-{}", i)),
                    None,
                )
                .await
                .unwrap();

            assert_eq!(result.topic, "test");
            assert!(result.partition < 3);
        }
    }

    #[tokio::test]
    async fn test_segment_rolling() {
        let (storage, _temp) = setup_test_env().await;

        // Write enough data to trigger segment roll (> 1MB)
        let large_value = vec![b'x'; 1024]; // 1KB per record
        for i in 0..2000 {
            // 2MB total
            storage
                .append("test", None, Bytes::from(large_value.clone()), None)
                .await
                .unwrap();
        }

        storage.flush_all().await.unwrap();

        // Should have created multiple segments
        // (verification would query metadata store)
    }
}
```

---

## Performance Targets

| Metric | Target | Notes |
|--------|--------|-------|
| Write throughput | 50,000 records/sec | Single partition |
| Write latency (p99) | < 10ms | Before S3 upload |
| Segment roll time | < 500ms | 64MB segment |
| S3 upload time | < 2s | 64MB, depends on network |

---

## Deliverables

At the end of Initiative 1.4, you will have:

- [ ] **PartitionWriter** managing single partition writes
- [ ] **TopicWriter** managing multi-partition topics
- [ ] **StorageManager** coordinating all writes
- [ ] **Segment rolling** based on size and time thresholds
- [ ] **S3 upload** with retry logic
- [ ] **Metadata updates** for segments and watermarks
- [ ] **Integration test** writing 100K records and verifying in S3

---

## Next Steps

After completing the Write Path (Initiative 1.4), you'll move to:

**Initiative 1.5: Read Path** - Implement consumer logic to read records from S3, with caching and prefetching for performance.

---

## Dependencies

```toml
[dependencies]
streamhouse-core = { path = "../streamhouse-core" }
streamhouse-metadata = { path = "../streamhouse-metadata" }
bytes = "1.5"
tokio = { version = "1.35", features = ["full"] }
object_store = { version = "0.9", features = ["aws"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
tracing = "0.1"
thiserror = "1.0"
```

---

## Time Estimate

**Total: 1-1.5 weeks** (assuming 25-35 hours of work)

- PartitionWriter implementation: 6-8 hours
- TopicWriter and StorageManager: 4-6 hours
- S3 integration and retry logic: 4-6 hours
- Configuration and error handling: 2-3 hours
- Testing and debugging: 6-8 hours
- Documentation: 3-4 hours
