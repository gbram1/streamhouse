# Initiative 1.3: Metadata Store

**Timeline:** Week 2-3
**Goal:** Build a metadata tracking system for topics, partitions, segments, and consumer offsets

---

## Overview

The **Metadata Store** is the "brain" of the streaming system. While segments store the actual event data in S3, the metadata store tracks *where* everything is, *what* exists, and *who* has consumed what.

Think of it as the card catalog in a library - it doesn't contain the books (data), but it tells you exactly which shelf to find them on.

---

## The Problem

### Without Metadata

Imagine you have thousands of segment files in S3:
```
s3://bucket/data/orders/0/00000000000000000000.seg
s3://bucket/data/orders/0/00000000000000100000.seg
s3://bucket/data/orders/0/00000000000000200000.seg
s3://bucket/data/orders/1/00000000000000000000.seg
... thousands more ...
```

**Questions you can't answer efficiently:**
- Which segment contains offset 150,000?
- What topics exist?
- How many partitions does "orders" have?
- What's the latest offset in partition 2?
- Where did consumer group "analytics" leave off?

**Bad approach:** List all S3 files and parse names
- ❌ S3 LIST is slow (seconds for thousands of files)
- ❌ Must parse filenames to extract offsets
- ❌ No way to track consumer positions
- ❌ Doesn't scale

**Our approach:** Central metadata store
- ✅ Instant lookups (microseconds)
- ✅ Structured queries
- ✅ Transactional updates
- ✅ Scalable (can move to Postgres/FoundationDB later)

---

## What We're Building

### Data Model

```
Topics
  ├── Partitions
  │     ├── High Watermark (latest offset)
  │     └── Segments
  │           ├── Base Offset
  │           ├── End Offset
  │           ├── S3 Path
  │           └── Size
  └── Consumer Groups
        └── Committed Offsets (per partition)
```

### Example Data

```
Topic: "orders"
├── Partition 0
│   ├── High Watermark: 199,999
│   └── Segments:
│       ├── Segment 1: offsets 0-99,999 → s3://.../00000000000000000000.seg
│       └── Segment 2: offsets 100,000-199,999 → s3://.../00000000000000100000.seg
├── Partition 1
│   ├── High Watermark: 150,000
│   └── Segments:
│       └── Segment 1: offsets 0-150,000 → s3://.../00000000000000000000.seg
└── Consumer Group "analytics"
    ├── Partition 0: offset 180,000
    └── Partition 1: offset 140,000
```

---

## Database Schema

### Technology Choice: SQLite (Phase 1)

**Why SQLite for Phase 1?**
- ✅ Zero configuration (embedded)
- ✅ ACID transactions
- ✅ Perfect for single-node deployment
- ✅ Easy to migrate to Postgres later
- ✅ Handles 100K+ queries/sec

**Migration Path:**
- Phase 1: SQLite (single node)
- Phase 4: PostgreSQL (distributed) or FoundationDB (for scale)

### Schema Design

```sql
-- ============================================================
-- TOPICS
-- ============================================================
CREATE TABLE topics (
    name TEXT PRIMARY KEY,
    partition_count INTEGER NOT NULL CHECK(partition_count > 0),
    retention_ms BIGINT,                  -- NULL = infinite retention
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    config JSON                           -- Topic-specific config
);

CREATE INDEX idx_topics_created_at ON topics(created_at);

-- ============================================================
-- PARTITIONS
-- ============================================================
CREATE TABLE partitions (
    topic TEXT NOT NULL,
    partition_id INTEGER NOT NULL,
    high_watermark BIGINT NOT NULL DEFAULT 0,  -- Latest committed offset
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (topic, partition_id),
    FOREIGN KEY (topic) REFERENCES topics(name) ON DELETE CASCADE
);

CREATE INDEX idx_partitions_topic ON partitions(topic);

-- ============================================================
-- SEGMENTS
-- ============================================================
CREATE TABLE segments (
    id TEXT PRIMARY KEY,                 -- UUID or {topic}-{partition}-{base_offset}
    topic TEXT NOT NULL,
    partition_id INTEGER NOT NULL,
    base_offset BIGINT NOT NULL,         -- First offset in segment
    end_offset BIGINT NOT NULL,          -- Last offset in segment
    record_count INTEGER NOT NULL,
    size_bytes BIGINT NOT NULL,
    s3_bucket TEXT NOT NULL,
    s3_key TEXT NOT NULL,                -- S3 object key
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (topic, partition_id) REFERENCES partitions(topic, partition_id) ON DELETE CASCADE,
    CHECK (end_offset >= base_offset)
);

CREATE UNIQUE INDEX idx_segments_location ON segments(topic, partition_id, base_offset);
CREATE INDEX idx_segments_s3_path ON segments(s3_bucket, s3_key);
CREATE INDEX idx_segments_offsets ON segments(topic, partition_id, base_offset, end_offset);

-- ============================================================
-- CONSUMER GROUPS & OFFSETS
-- ============================================================
CREATE TABLE consumer_groups (
    group_id TEXT PRIMARY KEY,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE consumer_offsets (
    group_id TEXT NOT NULL,
    topic TEXT NOT NULL,
    partition_id INTEGER NOT NULL,
    committed_offset BIGINT NOT NULL,
    metadata TEXT,                       -- Optional: consumer metadata
    committed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (group_id, topic, partition_id),
    FOREIGN KEY (group_id) REFERENCES consumer_groups(group_id) ON DELETE CASCADE,
    FOREIGN KEY (topic, partition_id) REFERENCES partitions(topic, partition_id) ON DELETE CASCADE
);

CREATE INDEX idx_consumer_offsets_group ON consumer_offsets(group_id);
CREATE INDEX idx_consumer_offsets_topic ON consumer_offsets(topic, partition_id);
```

---

## Rust Implementation

### Trait Definition

```rust
// crates/streamhouse-metadata/src/lib.rs

use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TopicConfig {
    pub name: String,
    pub partition_count: u32,
    pub retention_ms: Option<i64>,
    pub config: HashMap<String, String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Topic {
    pub name: String,
    pub partition_count: u32,
    pub retention_ms: Option<i64>,
    pub created_at: i64,
    pub config: HashMap<String, String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Partition {
    pub topic: String,
    pub partition_id: u32,
    pub high_watermark: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SegmentInfo {
    pub id: String,
    pub topic: String,
    pub partition_id: u32,
    pub base_offset: u64,
    pub end_offset: u64,
    pub record_count: u32,
    pub size_bytes: u64,
    pub s3_bucket: String,
    pub s3_key: String,
    pub created_at: i64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ConsumerOffset {
    pub group_id: String,
    pub topic: String,
    pub partition_id: u32,
    pub committed_offset: u64,
    pub metadata: Option<String>,
}

/// Metadata store trait - can be implemented with SQLite, Postgres, etc.
#[async_trait]
pub trait MetadataStore: Send + Sync {
    // ============================================================
    // TOPIC OPERATIONS
    // ============================================================

    /// Create a new topic
    async fn create_topic(&self, config: TopicConfig) -> Result<()>;

    /// Delete a topic and all its data
    async fn delete_topic(&self, name: &str) -> Result<()>;

    /// Get topic information
    async fn get_topic(&self, name: &str) -> Result<Option<Topic>>;

    /// List all topics
    async fn list_topics(&self) -> Result<Vec<Topic>>;

    /// Update topic configuration
    async fn update_topic_config(&self, name: &str, config: HashMap<String, String>) -> Result<()>;

    // ============================================================
    // PARTITION OPERATIONS
    // ============================================================

    /// Get partition information
    async fn get_partition(&self, topic: &str, partition_id: u32) -> Result<Option<Partition>>;

    /// Update high watermark (latest offset) for a partition
    async fn update_high_watermark(&self, topic: &str, partition_id: u32, offset: u64) -> Result<()>;

    /// Get all partitions for a topic
    async fn list_partitions(&self, topic: &str) -> Result<Vec<Partition>>;

    // ============================================================
    // SEGMENT OPERATIONS
    // ============================================================

    /// Add a new segment
    async fn add_segment(&self, segment: SegmentInfo) -> Result<()>;

    /// Get all segments for a partition
    async fn get_segments(&self, topic: &str, partition_id: u32) -> Result<Vec<SegmentInfo>>;

    /// Find the segment containing a specific offset
    async fn find_segment_for_offset(
        &self,
        topic: &str,
        partition_id: u32,
        offset: u64,
    ) -> Result<Option<SegmentInfo>>;

    /// Delete old segments (for retention policy)
    async fn delete_segments_before(&self, topic: &str, partition_id: u32, offset: u64) -> Result<u64>;

    // ============================================================
    // CONSUMER GROUP OPERATIONS
    // ============================================================

    /// Create or get a consumer group
    async fn ensure_consumer_group(&self, group_id: &str) -> Result<()>;

    /// Commit an offset for a consumer group
    async fn commit_offset(
        &self,
        group_id: &str,
        topic: &str,
        partition_id: u32,
        offset: u64,
        metadata: Option<String>,
    ) -> Result<()>;

    /// Get committed offset for a consumer group
    async fn get_committed_offset(
        &self,
        group_id: &str,
        topic: &str,
        partition_id: u32,
    ) -> Result<Option<u64>>;

    /// Get all offsets for a consumer group
    async fn get_consumer_offsets(&self, group_id: &str) -> Result<Vec<ConsumerOffset>>;

    /// Delete a consumer group
    async fn delete_consumer_group(&self, group_id: &str) -> Result<()>;
}

pub type Result<T> = std::result::Result<T, MetadataError>;

#[derive(Debug, thiserror::Error)]
pub enum MetadataError {
    #[error("Topic not found: {0}")]
    TopicNotFound(String),

    #[error("Topic already exists: {0}")]
    TopicAlreadyExists(String),

    #[error("Partition not found: {topic}/{partition}")]
    PartitionNotFound { topic: String, partition: u32 },

    #[error("Database error: {0}")]
    DatabaseError(#[from] sqlx::Error),

    #[error("Invalid offset: {0}")]
    InvalidOffset(String),
}
```

### SQLite Implementation

```rust
// crates/streamhouse-metadata/src/sqlite.rs

use sqlx::sqlite::{SqlitePool, SqlitePoolOptions};
use std::path::Path;
use std::sync::Arc;

pub struct SqliteMetadataStore {
    pool: SqlitePool,
}

impl SqliteMetadataStore {
    /// Create a new SQLite metadata store
    pub async fn new<P: AsRef<Path>>(path: P) -> Result<Self> {
        let url = format!("sqlite://{}?mode=rwc", path.as_ref().display());

        let pool = SqlitePoolOptions::new()
            .max_connections(10)
            .connect(&url)
            .await?;

        // Run migrations
        sqlx::migrate!("./migrations")
            .run(&pool)
            .await?;

        Ok(Self { pool })
    }

    /// Create in-memory database (for testing)
    pub async fn new_in_memory() -> Result<Self> {
        let pool = SqlitePoolOptions::new()
            .max_connections(10)
            .connect("sqlite::memory:")
            .await?;

        sqlx::migrate!("./migrations")
            .run(&pool)
            .await?;

        Ok(Self { pool })
    }
}

#[async_trait]
impl MetadataStore for SqliteMetadataStore {
    async fn create_topic(&self, config: TopicConfig) -> Result<()> {
        let mut tx = self.pool.begin().await?;

        // Insert topic
        let config_json = serde_json::to_string(&config.config).unwrap();
        sqlx::query!(
            r#"
            INSERT INTO topics (name, partition_count, retention_ms, config)
            VALUES (?, ?, ?, ?)
            "#,
            config.name,
            config.partition_count,
            config.retention_ms,
            config_json,
        )
        .execute(&mut *tx)
        .await
        .map_err(|e| {
            if e.to_string().contains("UNIQUE constraint failed") {
                MetadataError::TopicAlreadyExists(config.name.clone())
            } else {
                e.into()
            }
        })?;

        // Create partitions
        for partition_id in 0..config.partition_count {
            sqlx::query!(
                r#"
                INSERT INTO partitions (topic, partition_id, high_watermark)
                VALUES (?, ?, 0)
                "#,
                config.name,
                partition_id,
            )
            .execute(&mut *tx)
            .await?;
        }

        tx.commit().await?;
        Ok(())
    }

    async fn get_topic(&self, name: &str) -> Result<Option<Topic>> {
        let row = sqlx::query!(
            r#"
            SELECT name, partition_count, retention_ms, created_at, config
            FROM topics
            WHERE name = ?
            "#,
            name,
        )
        .fetch_optional(&self.pool)
        .await?;

        Ok(row.map(|r| Topic {
            name: r.name,
            partition_count: r.partition_count as u32,
            retention_ms: r.retention_ms,
            created_at: r.created_at.unwrap_or(0),
            config: serde_json::from_str(&r.config.unwrap_or_default()).unwrap_or_default(),
        }))
    }

    async fn find_segment_for_offset(
        &self,
        topic: &str,
        partition_id: u32,
        offset: u64,
    ) -> Result<Option<SegmentInfo>> {
        let offset_i64 = offset as i64;
        let row = sqlx::query!(
            r#"
            SELECT id, topic, partition_id, base_offset, end_offset,
                   record_count, size_bytes, s3_bucket, s3_key, created_at
            FROM segments
            WHERE topic = ?
              AND partition_id = ?
              AND base_offset <= ?
              AND end_offset >= ?
            ORDER BY base_offset DESC
            LIMIT 1
            "#,
            topic,
            partition_id,
            offset_i64,
            offset_i64,
        )
        .fetch_optional(&self.pool)
        .await?;

        Ok(row.map(|r| SegmentInfo {
            id: r.id,
            topic: r.topic,
            partition_id: r.partition_id as u32,
            base_offset: r.base_offset as u64,
            end_offset: r.end_offset as u64,
            record_count: r.record_count as u32,
            size_bytes: r.size_bytes as u64,
            s3_bucket: r.s3_bucket,
            s3_key: r.s3_key,
            created_at: r.created_at.unwrap_or(0),
        }))
    }

    async fn commit_offset(
        &self,
        group_id: &str,
        topic: &str,
        partition_id: u32,
        offset: u64,
        metadata: Option<String>,
    ) -> Result<()> {
        let offset_i64 = offset as i64;

        // Ensure group exists
        sqlx::query!(
            "INSERT OR IGNORE INTO consumer_groups (group_id) VALUES (?)",
            group_id,
        )
        .execute(&self.pool)
        .await?;

        // Upsert offset
        sqlx::query!(
            r#"
            INSERT INTO consumer_offsets (group_id, topic, partition_id, committed_offset, metadata)
            VALUES (?, ?, ?, ?, ?)
            ON CONFLICT(group_id, topic, partition_id)
            DO UPDATE SET
                committed_offset = excluded.committed_offset,
                metadata = excluded.metadata,
                committed_at = CURRENT_TIMESTAMP
            "#,
            group_id,
            topic,
            partition_id,
            offset_i64,
            metadata,
        )
        .execute(&self.pool)
        .await?;

        Ok(())
    }

    // ... implement other trait methods similarly ...
}
```

---

## Usage Examples

### Topic Management

```rust
use streamhouse_metadata::{MetadataStore, SqliteMetadataStore, TopicConfig};

#[tokio::main]
async fn main() -> Result<()> {
    let store = SqliteMetadataStore::new("metadata.db").await?;

    // Create a topic
    store.create_topic(TopicConfig {
        name: "orders".to_string(),
        partition_count: 3,
        retention_ms: Some(7 * 24 * 60 * 60 * 1000), // 7 days
        config: HashMap::new(),
    }).await?;

    // List topics
    let topics = store.list_topics().await?;
    for topic in topics {
        println!("Topic: {} ({} partitions)", topic.name, topic.partition_count);
    }

    // Get specific topic
    if let Some(topic) = store.get_topic("orders").await? {
        println!("Found topic: {:?}", topic);
    }

    Ok(())
}
```

### Segment Tracking

```rust
// When a segment is written to S3
let segment = SegmentInfo {
    id: format!("orders-0-{}", base_offset),
    topic: "orders".to_string(),
    partition_id: 0,
    base_offset: 0,
    end_offset: 99_999,
    record_count: 100_000,
    size_bytes: 67_108_864, // 64MB
    s3_bucket: "streamhouse".to_string(),
    s3_key: "data/orders/0/00000000000000000000.seg".to_string(),
    created_at: now_ms(),
};

store.add_segment(segment).await?;

// Later, when reading from offset 50,000
let segment = store
    .find_segment_for_offset("orders", 0, 50_000)
    .await?
    .expect("Segment should exist");

println!("Download from: s3://{}/{}", segment.s3_bucket, segment.s3_key);
```

### Consumer Offsets

```rust
// Consumer commits progress
store.commit_offset(
    "analytics-group",
    "orders",
    0,
    50_000,
    Some("processed up to order #50000".to_string()),
).await?;

// Consumer restarts and resumes
let last_offset = store
    .get_committed_offset("analytics-group", "orders", 0)
    .await?
    .unwrap_or(0);

println!("Resuming from offset: {}", last_offset);
```

---

## Migrations

Create migration files in `crates/streamhouse-metadata/migrations/`:

**`001_initial_schema.sql`:**
```sql
-- Create initial tables
-- (paste the full schema from earlier)
```

**`002_add_indexes.sql`:**
```sql
-- Additional indexes for performance
CREATE INDEX IF NOT EXISTS idx_segments_created_at ON segments(created_at);
CREATE INDEX IF NOT EXISTS idx_consumer_offsets_committed_at ON consumer_offsets(committed_at);
```

---

## Testing

### Unit Tests

```rust
#[cfg(test)]
mod tests {
    use super::*;

    async fn setup_test_store() -> SqliteMetadataStore {
        SqliteMetadataStore::new_in_memory().await.unwrap()
    }

    #[tokio::test]
    async fn test_create_and_get_topic() {
        let store = setup_test_store().await;

        let config = TopicConfig {
            name: "test".to_string(),
            partition_count: 3,
            retention_ms: None,
            config: HashMap::new(),
        };

        store.create_topic(config.clone()).await.unwrap();

        let topic = store.get_topic("test").await.unwrap().unwrap();
        assert_eq!(topic.name, "test");
        assert_eq!(topic.partition_count, 3);

        // Should have created 3 partitions
        let partitions = store.list_partitions("test").await.unwrap();
        assert_eq!(partitions.len(), 3);
    }

    #[tokio::test]
    async fn test_duplicate_topic_fails() {
        let store = setup_test_store().await;

        let config = TopicConfig {
            name: "test".to_string(),
            partition_count: 1,
            retention_ms: None,
            config: HashMap::new(),
        };

        store.create_topic(config.clone()).await.unwrap();

        // Second create should fail
        let result = store.create_topic(config).await;
        assert!(matches!(result, Err(MetadataError::TopicAlreadyExists(_))));
    }

    #[tokio::test]
    async fn test_find_segment_for_offset() {
        let store = setup_test_store().await;

        // Create topic
        store.create_topic(TopicConfig {
            name: "test".to_string(),
            partition_count: 1,
            retention_ms: None,
            config: HashMap::new(),
        }).await.unwrap();

        // Add segments
        store.add_segment(SegmentInfo {
            id: "seg1".to_string(),
            topic: "test".to_string(),
            partition_id: 0,
            base_offset: 0,
            end_offset: 999,
            record_count: 1000,
            size_bytes: 1024,
            s3_bucket: "test".to_string(),
            s3_key: "seg1.seg".to_string(),
            created_at: 0,
        }).await.unwrap();

        store.add_segment(SegmentInfo {
            id: "seg2".to_string(),
            topic: "test".to_string(),
            partition_id: 0,
            base_offset: 1000,
            end_offset: 1999,
            record_count: 1000,
            size_bytes: 1024,
            s3_bucket: "test".to_string(),
            s3_key: "seg2.seg".to_string(),
            created_at: 0,
        }).await.unwrap();

        // Find offset in first segment
        let seg = store.find_segment_for_offset("test", 0, 500).await.unwrap().unwrap();
        assert_eq!(seg.id, "seg1");

        // Find offset in second segment
        let seg = store.find_segment_for_offset("test", 0, 1500).await.unwrap().unwrap();
        assert_eq!(seg.id, "seg2");

        // Offset doesn't exist yet
        let seg = store.find_segment_for_offset("test", 0, 5000).await.unwrap();
        assert!(seg.is_none());
    }

    #[tokio::test]
    async fn test_consumer_offset_commit() {
        let store = setup_test_store().await;

        // Create topic
        store.create_topic(TopicConfig {
            name: "test".to_string(),
            partition_count: 1,
            retention_ms: None,
            config: HashMap::new(),
        }).await.unwrap();

        // Initially no offset
        let offset = store.get_committed_offset("group1", "test", 0).await.unwrap();
        assert!(offset.is_none());

        // Commit offset
        store.commit_offset("group1", "test", 0, 100, None).await.unwrap();

        // Should be retrievable
        let offset = store.get_committed_offset("group1", "test", 0).await.unwrap();
        assert_eq!(offset, Some(100));

        // Update offset
        store.commit_offset("group1", "test", 0, 200, None).await.unwrap();
        let offset = store.get_committed_offset("group1", "test", 0).await.unwrap();
        assert_eq!(offset, Some(200));
    }
}
```

---

## Performance Considerations

### Query Performance

**Fast Queries (< 1ms):**
- Get topic by name (indexed primary key)
- Get partition info (composite index)
- Find segment for offset (index on topic + partition + offsets)
- Get consumer offset (composite primary key)

**Slower Queries (1-10ms):**
- List all topics (full table scan, but small)
- List all segments for partition (filtered scan)

**Optimization Tips:**
```sql
-- Always use prepared statements (sqlx does this)
-- Add indexes for common query patterns
-- Use EXPLAIN QUERY PLAN to verify index usage

EXPLAIN QUERY PLAN
SELECT * FROM segments
WHERE topic = 'orders' AND partition_id = 0 AND base_offset <= 1000 AND end_offset >= 1000;
```

### Write Performance

SQLite can handle:
- 50,000+ inserts/sec with transactions
- 100,000+ reads/sec

**Best Practices:**
- Batch updates in transactions
- Use `INSERT OR IGNORE` / `INSERT ... ON CONFLICT` for upserts
- VACUUM database periodically

---

## Deliverables

At the end of Initiative 1.3, you will have:

- [ ] **MetadataStore trait** defined with all necessary operations
- [ ] **SQLite implementation** fully functional
- [ ] **Database schema** with proper indexes
- [ ] **Migration system** using sqlx-cli
- [ ] **Unit tests** covering all operations
- [ ] **Integration tests** with real SQLite database
- [ ] **Documentation** on schema and API usage

---

## Next Steps

After completing the Metadata Store (Initiative 1.3), you'll move to:

**Initiative 1.4: Write Path** - Use the metadata store to track segments as they're written to S3, update high watermarks, and coordinate the full write flow.

---

## Dependencies

```toml
[dependencies]
sqlx = { version = "0.7", features = ["runtime-tokio", "sqlite", "migrate"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
async-trait = "0.1"
thiserror = "1.0"
tokio = { version = "1.35", features = ["full"] }
```

---

## Time Estimate

**Total: 1-1.5 weeks** (assuming 25-35 hours of work)

- Schema design: 3-4 hours
- Trait definition: 2-3 hours
- SQLite implementation: 8-12 hours
- Testing: 6-8 hours
- Documentation: 3-4 hours
- Debugging and refinement: 3-5 hours
