# StreamHouse - Deferred Work & Future Tasks

**Last Updated**: 2026-01-23
**Purpose**: Track features/improvements that are valuable but not critical path

---

## Phase 3 Optional Items (Deferred Until After Phase 4)

### Phase 3.5: CockroachDB Backend

**Status**: ‚è≥ Deferred
**Priority**: Medium
**When**: After Phase 4 complete, if we hit PostgreSQL limits

**What**: Implement `CockroachMetadataStore` for distributed metadata

**Why defer**:
- PostgreSQL + caching handles 10,000 partitions fine
- Not needed until we hit 100,000+ partitions
- Can add later without breaking changes (trait already defined)

**When to revisit**:
- Single PostgreSQL instance becomes bottleneck
- Need multi-region active-active metadata
- Customer requires 100K+ partition workload

**Implementation checklist**:
```
[ ] Create CockroachMetadataStore struct
[ ] Implement all 21 MetadataStore trait methods
[ ] Write CockroachDB-specific SQL migrations
[ ] Add distributed transaction handling
[ ] Integration tests
[ ] Performance benchmarks (compare to PostgreSQL)
[ ] Documentation
```

**Estimated effort**: 2 weeks

---

### Phase 3.6: DynamoDB Backend

**Status**: ‚è≥ Deferred
**Priority**: Low
**When**: For AWS-native deployments

**What**: Implement `DynamoDbMetadataStore` for serverless metadata

**Why defer**:
- PostgreSQL works everywhere (not AWS-specific)
- DynamoDB requires different query patterns (no JOINs)
- Cost/benefit unclear (need customer demand)

**When to revisit**:
- Customer requires AWS-native solution (no RDS)
- Want pay-per-request pricing model
- Need unlimited scale (>1M partitions)

**Implementation checklist**:
```
[ ] Design DynamoDB table schema (single-table design)
[ ] Create DynamoDbMetadataStore struct
[ ] Implement all 21 MetadataStore trait methods
[ ] Handle eventual consistency
[ ] Add GSI (Global Secondary Indexes) for queries
[ ] Integration tests with LocalStack
[ ] Cost modeling (vs RDS)
[ ] Documentation
```

**Estimated effort**: 3 weeks

**Challenges**:
- DynamoDB is eventually consistent (partition leases need strong consistency)
- No JOINs (need to denormalize data)
- Different query patterns (need careful GSI design)

---

### Phase 3.7: Pathological Workload Testing

**Status**: ‚è≥ Deferred
**Priority**: Medium
**When**: After Phase 4 multi-agent is stable

**What**: Test extreme scenarios (1000 partitions √ó 1000 tiny batches)

**Why defer**:
- Current demo (1,650 records) proves correctness
- Need multi-agent first to test scalability properly
- Can add stress tests anytime

**When to revisit**:
- Phase 4 complete (multi-agent running)
- Before marketing "handles pathological workloads"
- Customer reports performance issues

**Test scenarios**:
```rust
// Scenario 1: Many partitions, tiny batches
for partition in 0..1000 {
    for batch in 0..1000 {
        produce_record(topic, partition, 10_bytes);
    }
}
// Goal: Metadata cache hit rate > 90%

// Scenario 2: Bursty traffic
for _ in 0..10 {
    produce_burst(10_000 records);
    sleep(60 seconds);
}
// Goal: No bufferbloat, consistent latency

// Scenario 3: Hot partition
for _ in 0..1_000_000 {
    produce_record(topic, partition=0, 1KB);
}
// Goal: Single partition handles high throughput
```

**Success criteria**:
- Metadata query latency < 10ms p99
- Cache hit rate > 90%
- End-to-end write latency < 500ms p99
- No database overload

**Estimated effort**: 1 week

---

### Phase 3.8: Cross-Partition Batching

**Status**: ‚è≥ Deferred
**Priority**: Low
**When**: For cost optimization at scale

**What**: Batch records from multiple partitions into single S3 object

**Current state**:
```
orders/0/segment.bin  (100 records)
orders/1/segment.bin  (150 records)
orders/2/segment.bin  (80 records)
Total: 3 S3 PUT requests
```

**Optimized state**:
```
orders/batch-00001.bin  (330 records across 3 partitions)
Total: 1 S3 PUT request
```

**Why defer**:
- Current approach works fine
- Adds complexity (need to track partition boundaries within batches)
- Only matters at high scale (>10,000 partitions)

**When to revisit**:
- S3 PUT costs become significant (>$50/month)
- Pathological workload testing reveals bottleneck
- Customer requires ultra-low-cost deployment

**Benefits**:
- Fewer S3 PUT requests = lower cost
- Better compression (larger batches)
- Amortized latency

**Challenges**:
- Need to track partition boundaries in batch
- Read path more complex (find partition within batch)
- Lease coordination trickier (who batches what?)

**Estimated effort**: 2 weeks

---

## Phase 2 Deferred Items

### Phase 2.2: Kafka Protocol Compatibility

**Status**: ‚è≥ Deferred (indefinitely)
**Priority**: Low
**When**: If customer demand requires drop-in Kafka replacement

**What**: Implement Kafka wire protocol for producer/consumer compatibility

**Why defer**:
- gRPC API sufficient for now
- Kafka protocol extremely complex (100+ message types)
- Can wrap StreamHouse with Kafka proxy if needed

**When to revisit**:
- Customer requires existing Kafka clients (no code changes)
- Marketing wants "Kafka-compatible" positioning
- Integration with Kafka ecosystem tools

**Alternatives**:
1. **Kafka Proxy**: Run separate proxy that translates Kafka ‚Üî StreamHouse gRPC
2. **Client Libraries**: Provide idiomatic clients (Python, Java, Go, Rust)
3. **Connectors**: Build specific connectors (Flink, Spark, etc.)

**Estimated effort**: 8 weeks (very large undertaking)

**Recommendation**: Build client libraries instead of full Kafka protocol

---

## Phase 6+ Future Features

### SQL Stream Processing (Phase 6)

**Status**: üìã Planned for later
**Priority**: High (our differentiation!)
**When**: After Phase 4 + Phase 5 complete

**What**: Built-in SQL engine for streaming queries

Already documented in [COMPLETE-ROADMAP.md](COMPLETE-ROADMAP.md)

Not deferred - just scheduled for after multi-agent is production-ready.

---

### Schema Registry (Phase 7+)

**Status**: üìã Planned
**Priority**: Medium
**When**: After SQL processing

**What**: Stateless schema registry for Avro/Protobuf/JSON

**Why later**:
- SQL processing more important (our differentiation)
- Can integrate with Confluent Schema Registry for now
- Need solid foundation first

**Alternatives** (interim):
- Use Confluent Schema Registry (compatible)
- Store schemas in topic configs (metadata store)
- Client-side schema validation

---

### Tableflow - Iceberg Integration (Phase 7+)

**Status**: üìã Planned
**Priority**: Medium
**When**: After Schema Registry

**What**: Automatic Iceberg table generation from topics

**Use case**: Query streaming data with SQL engines (Trino, Athena, Spark)

**Why later**: Need stable format first

---

### S3 Express One Zone Support (Phase 7+)

**Status**: üìã Planned
**Priority**: Low
**When**: For ultra-low latency use cases

**What**: Support S3 Express One Zone storage class

**Benefits**:
- 4x lower latency than S3 Standard
- 10x higher throughput
- Single-AZ deployment (no cross-AZ costs)

**Cost**:
- $0.16/GB vs $0.023/GB (7x more expensive)
- Only makes sense for hot data (< 30 days)

**When to revisit**:
- Customer requires < 10ms write latency
- Cost justifies benefit (high-value use case)

---

## Infrastructure & Operations

### Kubernetes Operator

**Status**: üìã Future
**Priority**: Medium
**When**: For easier K8s deployments

**What**: Custom K8s operator for StreamHouse

```yaml
apiVersion: streamhouse.io/v1
kind: StreamHouseCluster
metadata:
  name: prod-cluster
spec:
  agents:
    replicas: 10
    instanceType: c6i.xlarge
  metadata:
    type: rds-postgresql
  storage:
    type: s3
    bucket: streamhouse-prod
```

**Why later**: Manual K8s manifests work fine for now

---

### Web UI Dashboard

**Status**: üìã Future
**Priority**: High (for managed service)
**When**: Before commercial launch

**What**: Customer-facing dashboard

**Features**:
- Topic management (create, configure, delete)
- Consumer group monitoring
- Usage analytics
- Query builder (SQL)
- Billing

**Not needed**: Until we have customers

---

### Multi-Region Replication (Orbit)

**Status**: üìã Future
**Priority**: Low
**When**: For disaster recovery or multi-region active-active

**What**: Cross-region topic replication

**Use cases**:
- Disaster recovery (backup region)
- Multi-region active-active (low latency everywhere)
- Data sovereignty (EU data in EU region)

**Complexity**: High (conflict resolution, offset translation)

---

## Documentation Gaps

### Missing Docs (To Write Later)

1. **Architecture Deep Dive** (after Phase 4)
   - How leases work
   - Failure scenarios
   - Performance tuning

2. **Operations Guide** (after Phase 4)
   - Deployment patterns
   - Monitoring setup
   - Troubleshooting

3. **API Reference** (after Phase 6)
   - Complete gRPC API docs
   - SQL query reference
   - Configuration options

4. **Tutorials** (after Phase 6)
   - Getting started
   - Common patterns
   - Integration guides

---

## Performance Optimizations

### Potential Future Optimizations

1. **Zero-Copy Reads** (Low priority)
   - Use io_uring for S3 downloads
   - Avoid buffer copies in hot path
   - Estimated gain: 10-20% read throughput

2. **Custom Compression** (Low priority)
   - Evaluate Zstandard vs LZ4
   - Train dictionaries for better compression
   - Estimated gain: 20-30% storage savings

3. **Segment Index Optimization** (Medium priority)
   - Use bloom filters for key lookups
   - Cache hot segments in memory
   - Estimated gain: 50% reduction in S3 API calls

4. **Connection Pooling** (Medium priority)
   - Reuse gRPC connections between agents
   - Reduce connection overhead
   - Estimated gain: 5-10% latency reduction

---

## Monitoring & Alerting

### Enhanced Observability (Future)

**Current state**: Basic metrics (CPU, memory, throughput)

**Future enhancements**:
1. Distributed tracing (OpenTelemetry)
2. Request flamegraphs (performance profiling)
3. Query cost tracking (per customer)
4. Anomaly detection (ML-based alerting)
5. Capacity planning dashboard

**When**: After Phase 5 (production hardening)

---

## How to Use This Document

**When adding deferred work**:
1. Add section with clear title
2. Mark priority (High/Medium/Low)
3. Explain why deferred
4. Define "when to revisit" criteria
5. Estimate effort
6. Link to related docs

**When revisiting**:
1. Check "when to revisit" conditions
2. Evaluate current priority
3. Create implementation plan
4. Move to active roadmap
5. Update status here

---

## Summary

**Deferred ‚â† Canceled**

These are all valuable features that we'll build eventually. We're just being strategic about sequencing:

1. **Phase 4** (now): Multi-agent architecture
2. **Phase 5**: Production hardening
3. **Phase 6**: SQL processing (our differentiation!)
4. **Phase 7+**: Everything else

The deferred items mostly fall into:
- **Nice-to-haves** (DynamoDB backend, Kafka protocol)
- **Premature optimizations** (cross-partition batching)
- **Ecosystem features** (schema registry, Tableflow)

We'll circle back when:
- Customer demand justifies it
- Core platform is stable
- We have bandwidth

**Focus now**: Ship Phase 4, achieve WarpStream parity, then differentiate with SQL processing! üöÄ

---

**Review cadence**: Every 3 months, revisit priorities based on:
- Customer feedback
- Performance bottlenecks discovered
- Competitive landscape changes
- Engineering bandwidth available
