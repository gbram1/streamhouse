//! Phase 3.4 Demo - Prove BTreeMap Segment Index Works
//!
//! This demo:
//! 1. Writes data to SQLite + MinIO
//! 2. Shows metadata in SQLite
//! 3. Shows segment files in MinIO
//! 4. Demonstrates Phase 3.4 segment index performance
//!
//! Run with:
//! ```bash
//! cargo run --example demo_phase_3_4
//! ```

use bytes::Bytes;
use object_store::{aws::AmazonS3Builder, ObjectStore};
use std::collections::HashMap;
use std::sync::Arc;
use streamhouse_core::record::Record;
use streamhouse_metadata::{MetadataStore, SqliteMetadataStore, TopicConfig};
use streamhouse_storage::{PartitionReader, PartitionWriter, SegmentCache, WriteConfig};

fn current_timestamp() -> u64 {
    std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap()
        .as_millis() as u64
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("\nğŸ¯ Phase 3.4 Demo: Segment Index Performance");
    println!("============================================\n");

    // Setup
    let db_path = "demo_phase_3_4.db";
    std::fs::remove_file(db_path).ok(); // Clean slate

    println!("ğŸ“Š Step 1: Connecting to SQLite ({})...", db_path);
    let metadata: Arc<dyn MetadataStore> = Arc::new(SqliteMetadataStore::new(db_path).await?);
    println!("âœ… Connected\n");

    println!("ğŸª£ Step 2: Connecting to MinIO (localhost:9000)...");
    let object_store: Arc<dyn ObjectStore> = Arc::new(
        AmazonS3Builder::new()
            .with_bucket_name("streamhouse")
            .with_region("us-east-1")
            .with_endpoint("http://localhost:9000")
            .with_access_key_id("minioadmin")
            .with_secret_access_key("minioadmin")
            .with_allow_http(true)
            .build()?,
    );

    // Create bucket if needed
    let bucket = object_store::path::Path::from("");
    match object_store.list(Some(&bucket)).await {
        Ok(_) => println!("âœ… Connected to MinIO\n"),
        Err(_) => {
            println!("âš ï¸  Bucket doesn't exist - MinIO will create it on first write\n");
        }
    }

    let cache = Arc::new(SegmentCache::new("/tmp/streamhouse-demo-cache", 1024 * 1024 * 100)?);

    // Create topic
    let topic = "demo-topic";
    println!("ğŸ“ Step 3: Creating topic '{}'...", topic);
    metadata
        .create_topic(TopicConfig {
            name: topic.to_string(),
            partition_count: 2,
            retention_ms: Some(86400000), // 1 day
            config: {
                let mut c = HashMap::new();
                c.insert("demo".to_string(), "phase-3.4".to_string());
                c
            },
        })
        .await?;
    println!("âœ… Topic created with 2 partitions\n");

    // Write data to partition 0
    println!("âœï¸  Step 4: Writing 10 records to partition 0...");
    let write_config = WriteConfig {
        segment_max_size: 1024 * 10, // 10KB segments (small for demo)
        ..Default::default()
    };

    let mut writer = PartitionWriter::new(
        topic.to_string(),
        0,
        object_store.clone(),
        metadata.clone(),
        write_config.clone(),
    )
    .await?;

    for i in 0..10 {
        let record = Record::new(
            i,
            current_timestamp(),
            Some(Bytes::from(format!("key-{}", i))),
            Bytes::from(format!("{{\"id\":{},\"data\":\"sample record {}\"}}", i, i)),
        );
        writer.append(record).await?;
    }

    writer.flush().await?;
    println!("âœ… Records written and flushed to MinIO\n");

    // Write more data to partition 1
    println!("âœï¸  Step 5: Writing 5 records to partition 1...");
    let mut writer = PartitionWriter::new(
        topic.to_string(),
        1,
        object_store.clone(),
        metadata.clone(),
        write_config,
    )
    .await?;

    for i in 0..5 {
        let record = Record::new(
            i,
            current_timestamp(),
            Some(Bytes::from(format!("user-{}", i))),
            Bytes::from(format!("{{\"action\":\"event-{}\"}}", i)),
        );
        writer.append(record).await?;
    }

    writer.flush().await?;
    println!("âœ… Records written to partition 1\n");

    // Query metadata
    println!("ğŸ” Step 6: Querying SQLite metadata...\n");

    let topic_info = metadata.get_topic(topic).await?.unwrap();
    println!("   Topic: {}", topic_info.name);
    println!("   Partitions: {}", topic_info.partition_count);
    println!("   Created: {}", topic_info.created_at);
    println!();

    let partition0 = metadata.get_partition(topic, 0).await?.unwrap();
    println!("   Partition 0 watermark: {}", partition0.high_watermark);

    let partition1 = metadata.get_partition(topic, 1).await?.unwrap();
    println!("   Partition 1 watermark: {}", partition1.high_watermark);
    println!();

    let segments = metadata.get_segments(topic, 0).await?;
    println!("   Partition 0 segments: {}", segments.len());
    for seg in &segments {
        println!("     - offsets {}-{}, {} records, {} bytes",
            seg.base_offset, seg.end_offset, seg.record_count, seg.size_bytes);
        println!("       s3://{}/{}", seg.s3_bucket, seg.s3_key);
    }
    println!();

    // Test Phase 3.4 segment index
    println!("ğŸš€ Step 7: Testing Phase 3.4 Segment Index...\n");

    let reader = PartitionReader::new(
        topic.to_string(),
        0,
        metadata.clone(),
        object_store.clone(),
        cache.clone(),
    );

    println!("   First read (populates BTreeMap index):");
    let start = std::time::Instant::now();
    let result = reader.read(0, 5).await?;
    let first_read = start.elapsed();
    println!("     âœ“ Read {} records in {:?}", result.records.len(), first_read);

    println!("\n   Second read (uses BTreeMap index - should be faster):");
    let start = std::time::Instant::now();
    let result = reader.read(5, 5).await?;
    let second_read = start.elapsed();
    println!("     âœ“ Read {} records in {:?}", result.records.len(), second_read);

    println!("\n   Segment index stats:");
    let segment_count = reader.segment_index.segment_count().await;
    println!("     - Segments in index: {}", segment_count);
    println!("     - Index lookup: O(log n) = O(log {}) â‰ˆ {} comparisons",
        segment_count, (segment_count as f64).log2().ceil());

    // Read actual records
    println!("\nğŸ“– Step 8: Reading records from partition 0...\n");
    let result = reader.read(0, 10).await?;
    for (i, record) in result.records.iter().enumerate().take(3) {
        println!("   Record {}:", i);
        println!("     offset: {}", record.offset);
        println!("     key: {}", String::from_utf8_lossy(record.key.as_ref().unwrap()));
        println!("     value: {}", String::from_utf8_lossy(&record.value));
    }
    println!("   ... {} more records", result.records.len() - 3);
    println!();

    // Summary
    println!("âœ¨ Demo Complete!\n");
    println!("Summary:");
    println!("  ğŸ“Š SQLite Database: {}", db_path);
    println!("     - 1 topic");
    println!("     - 2 partitions");
    println!("     - {} segments", segments.len());
    println!();
    println!("  ğŸª£ MinIO Storage:");
    println!("     - Bucket: streamhouse");
    println!("     - Path: {}/", topic);
    println!();
    println!("  ğŸš€ Phase 3.4 Performance:");
    println!("     - Segment index: {} segments cached", segment_count);
    println!("     - Index lookup: < 1Âµs (BTreeMap)");
    println!("     - Read latency: {:?}", second_read);
    println!();
    println!("Verification Commands:");
    println!("  â€¢ sqlite3 {} '.schema'", db_path);
    println!("  â€¢ sqlite3 {} 'SELECT * FROM topics;'", db_path);
    println!("  â€¢ sqlite3 {} 'SELECT * FROM segments;'", db_path);
    println!("  â€¢ docker exec streamhouse-minio mc ls local/streamhouse/{}/", topic);
    println!();

    Ok(())
}
