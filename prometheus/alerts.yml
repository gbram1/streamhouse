# StreamHouse Alerting Rules

groups:
  - name: streamhouse_consumer_alerts
    interval: 30s
    rules:
      # High Consumer Lag (Warning)
      - alert: HighConsumerLag
        expr: streamhouse_consumer_lag_records > 10000
        for: 5m
        labels:
          severity: warning
          component: consumer
        annotations:
          summary: "High consumer lag detected on {{ $labels.instance }}"
          description: "Consumer group {{ $labels.group_id }} has {{ $value }} records lag on topic {{ $labels.topic }} partition {{ $labels.partition }}"

      # Very High Consumer Lag (Critical)
      - alert: CriticalConsumerLag
        expr: streamhouse_consumer_lag_records > 100000
        for: 5m
        labels:
          severity: critical
          component: consumer
        annotations:
          summary: "CRITICAL consumer lag on {{ $labels.instance }}"
          description: "Consumer group {{ $labels.group_id }} has {{ $value }} records lag on topic {{ $labels.topic }} partition {{ $labels.partition }}. Immediate action required!"

      # Consumer Lag Growing
      - alert: ConsumerLagGrowing
        expr: delta(streamhouse_consumer_lag_records[10m]) > 5000
        for: 10m
        labels:
          severity: warning
          component: consumer
        annotations:
          summary: "Consumer lag is growing on {{ $labels.instance }}"
          description: "Consumer group {{ $labels.group_id }} lag increased by {{ $value }} records in the last 10 minutes"

      # Consumer Stalled
      - alert: ConsumerStalled
        expr: rate(streamhouse_consumer_records_consumed_total[10m]) == 0
        for: 10m
        labels:
          severity: warning
          component: consumer
        annotations:
          summary: "Consumer is stalled on {{ $labels.instance }}"
          description: "Consumer group {{ $labels.group_id }} has not consumed any records for 10 minutes"

  - name: streamhouse_producer_alerts
    interval: 30s
    rules:
      # High Producer Error Rate
      - alert: HighProducerErrorRate
        expr: rate(streamhouse_producer_send_errors_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
          component: producer
        annotations:
          summary: "High producer error rate on {{ $labels.instance }}"
          description: "Producer on {{ $labels.instance }} has {{ $value }} errors/sec on topic {{ $labels.topic }}"

      # Critical Producer Error Rate
      - alert: CriticalProducerErrorRate
        expr: rate(streamhouse_producer_send_errors_total[5m]) > 100
        for: 2m
        labels:
          severity: critical
          component: producer
        annotations:
          summary: "CRITICAL producer error rate on {{ $labels.instance }}"
          description: "Producer on {{ $labels.instance }} has {{ $value }} errors/sec. Service degraded!"

      # High Producer Latency (P99)
      - alert: HighProducerLatency
        expr: histogram_quantile(0.99, rate(streamhouse_producer_send_duration_seconds_bucket[5m])) > 0.1
        for: 5m
        labels:
          severity: warning
          component: producer
        annotations:
          summary: "High producer latency on {{ $labels.instance }}"
          description: "P99 latency is {{ $value }}s on {{ $labels.instance }}"

      # Very High Producer Latency (P99)
      - alert: VeryHighProducerLatency
        expr: histogram_quantile(0.99, rate(streamhouse_producer_send_duration_seconds_bucket[5m])) > 1.0
        for: 2m
        labels:
          severity: critical
          component: producer
        annotations:
          summary: "CRITICAL producer latency on {{ $labels.instance }}"
          description: "P99 latency is {{ $value }}s on {{ $labels.instance }}. Performance severely degraded!"

      # Producer Throughput Drop
      - alert: ProducerThroughputDrop
        expr: rate(streamhouse_producer_records_sent_total[5m]) < (rate(streamhouse_producer_records_sent_total[30m] offset 30m) * 0.5)
        for: 10m
        labels:
          severity: warning
          component: producer
        annotations:
          summary: "Producer throughput dropped on {{ $labels.instance }}"
          description: "Producer throughput dropped by >50% on {{ $labels.instance }}"

  - name: streamhouse_agent_alerts
    interval: 30s
    rules:
      # Agent Down
      - alert: AgentDown
        expr: up{job="streamhouse-agents"} == 0
        for: 1m
        labels:
          severity: critical
          component: agent
        annotations:
          summary: "StreamHouse agent is down"
          description: "Agent {{ $labels.instance }} is not responding to scrapes"

      # Agent Not Ready
      - alert: AgentNotReady
        expr: up{job="streamhouse-agents"} == 1 and probe_success{job="streamhouse-agents",endpoint="/ready"} == 0
        for: 5m
        labels:
          severity: warning
          component: agent
        annotations:
          summary: "Agent is not ready"
          description: "Agent {{ $labels.instance }} is up but not ready (no active leases)"

      # No Active Partitions
      - alert: NoActivePartitions
        expr: streamhouse_agent_active_partitions == 0
        for: 5m
        labels:
          severity: warning
          component: agent
        annotations:
          summary: "Agent has no active partitions"
          description: "Agent {{ $labels.instance }} has no active partition leases for 5+ minutes"

      # High gRPC Error Rate
      - alert: HighAgentgRPCErrors
        expr: rate(streamhouse_agent_grpc_requests_total{status!="OK"}[5m]) > 10
        for: 5m
        labels:
          severity: warning
          component: agent
        annotations:
          summary: "High gRPC error rate on agent"
          description: "Agent {{ $labels.instance }} has {{ $value }} gRPC errors/sec"

      # High Write Latency
      - alert: HighAgentWriteLatency
        expr: histogram_quantile(0.99, rate(streamhouse_agent_write_latency_seconds_bucket[5m])) > 0.5
        for: 5m
        labels:
          severity: warning
          component: agent
        annotations:
          summary: "High write latency on agent"
          description: "Agent {{ $labels.instance }} P99 write latency is {{ $value }}s"

  - name: streamhouse_system_alerts
    interval: 30s
    rules:
      # High CPU Usage
      - alert: HighCPUUsage
        expr: 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 10m
        labels:
          severity: warning
          component: system
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value }}% on {{ $labels.instance }}"

      # High Memory Usage
      - alert: HighMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 90
        for: 5m
        labels:
          severity: warning
          component: system
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value }}% on {{ $labels.instance }}"

      # Disk Space Low
      - alert: DiskSpaceLow
        expr: (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100 < 10
        for: 5m
        labels:
          severity: critical
          component: system
        annotations:
          summary: "Disk space low on {{ $labels.instance }}"
          description: "Only {{ $value }}% disk space remaining on {{ $labels.instance }}"
