# StreamHouse

**A high-performance, S3-native streaming platform**

[![Build Status](https://img.shields.io/badge/build-passing-brightgreen)](.)
[![Tests](https://img.shields.io/badge/tests-59%20passing-brightgreen)](.)
[![License](https://img.shields.io/badge/license-MIT-blue)](LICENSE)

StreamHouse is an Apache Kafka alternative that stores events directly in S3, providing infinite scalability, 99.999999999% durability, and 10x cost reduction. Includes complete Producer and Consumer APIs with Kafka-compatible interfaces.

## Features

- ðŸš€ **High Performance**: 62K writes/sec, 30K+ reads/sec, < 10ms P99 metadata queries
- ðŸ’° **Cost Effective**: $0.023/GB/month (10x cheaper than Kafka)
- ðŸ“ˆ **Infinite Scale**: Stateless agents, S3 storage, horizontal scaling
- ðŸ”’ **Reliable**: 99.999999999% S3 durability, ACID metadata
- ðŸŽ¯ **Kafka-Compatible API**: Complete Producer and Consumer with batching, compression, consumer groups

## Quick Start (Docker Compose)

Get StreamHouse running in under 5 minutes:

```bash
# Clone and start
git clone https://github.com/streamhouse/streamhouse
cd streamhouse
docker compose up -d

# Verify it's running
curl http://localhost:8080/health

# Create a topic
curl -X POST http://localhost:8080/api/v1/topics \
  -H "Content-Type: application/json" \
  -d '{"name": "events", "partitions": 3}'

# Send a message
curl -X POST http://localhost:8080/api/v1/produce \
  -H "Content-Type: application/json" \
  -d '{"topic": "events", "key": "user-1", "value": "{\"event\": \"signup\"}"}'

# Open the Web UI
open http://localhost:3000
```

**Services available after startup:**

| Service | URL |
|---------|-----|
| REST API | http://localhost:8080 |
| Web UI | http://localhost:3000 |
| Grafana | http://localhost:3001 (admin/admin) |
| MinIO Console | http://localhost:9001 (minioadmin/minioadmin) |

For detailed instructions, see **[docs/QUICKSTART.md](docs/QUICKSTART.md)**.

---

## Rust Client Examples

### Producer API Example

```rust
use streamhouse_client::Producer;
use std::sync::Arc;

// Create producer
let producer = Producer::builder()
    .metadata_store(metadata_store)
    .agent_group("default")
    .batch_size(1000)
    .compression_enabled(true)
    .build()
    .await?;

// Send records (non-blocking)
let mut result = producer.send("orders", Some(b"user123"), b"order data", None).await?;

// Optional: wait for committed offset
let offset = result.wait_offset().await?;
println!("Record written at offset: {}", offset);

// Flush and close
producer.flush().await?;
producer.close().await?;
```

### Consumer API Example

```rust
use streamhouse_client::{Consumer, OffsetReset};
use std::time::Duration;

// Create consumer
let mut consumer = Consumer::builder()
    .group_id("analytics")
    .topics(vec!["orders".to_string()])
    .metadata_store(metadata_store)
    .object_store(object_store)
    .offset_reset(OffsetReset::Earliest)
    .auto_commit(true)
    .build()
    .await?;

// Poll for records
loop {
    let records = consumer.poll(Duration::from_secs(1)).await?;
    for record in records {
        println!("Offset: {}, Value: {:?}", record.offset, record.value);
    }
    consumer.commit().await?;
}
```

### Local Development with CLI

```bash
# 1. Start infrastructure
docker-compose up -d postgres

# 2. Set environment
export DATABASE_URL=postgres://streamhouse:streamhouse_dev@localhost:5432/streamhouse_metadata
export AWS_REGION=us-east-1
export S3_BUCKET=my-streamhouse-bucket

# 3. Run agent
cargo run --release --bin streamhouse-agent -- --port 50051

# 4. Use the CLI or API
cargo run --bin streamctl -- topic create orders --partitions 10
cargo run --example simple_producer
cargo run --example simple_consumer
```

## Documentation

### Architecture & Design
- **[Architecture Overview](docs/ARCHITECTURE_OVERVIEW.md)** - Complete system design
- **[Phases 1-3 Summary](docs/PHASES_1_TO_3_SUMMARY.md)** - Core platform implementation
- **[PostgreSQL Backend](docs/POSTGRES_BACKEND.md)** - Production deployment
- **[Metadata Caching](docs/METADATA_CACHING.md)** - Performance optimization

### API Documentation
- **[Phase 5: Producer API](docs/phases/phase5/README.md)** - Complete Producer implementation
  - [Phase 5.1: Core Client Library](docs/phases/phase5/5.1-summary.md)
  - [Phase 5.2: Producer Implementation](docs/phases/phase5/5.2-summary.md)
  - [Phase 5.3: gRPC Integration](docs/phases/phase5/5.3-summary.md)
  - [Phase 5.4: Offset Tracking](docs/phases/phase5/5.4-summary.md)
- **[Phase 6: Consumer API](docs/phases/phase6/README.md)** - Complete Consumer implementation
- **[Phase Index](docs/phases/INDEX.md)** - All phases overview
- **[Demo & Verification](DEMO.md)** - Quick start guide
- **[Phase 5+6 Complete](PHASE_5_6_COMPLETE.md)** - Full API summary

## Architecture

```
Producer API (batching, compression)
    â†“
StreamHouse Agent (gRPC, partition leases)
    â†“
S3 Storage (segments) + PostgreSQL Metadata (topics, offsets, consumer groups)
    â†“
Consumer API (multi-partition, offset management)
```

**Key Components**:
- **Producer API**: Batching, LZ4 compression, partition routing, retry logic, offset tracking
- **Consumer API**: Multi-partition fanout, consumer groups, auto-commit, direct S3 reads
- **Stateless Agents**: Buffer writes, manage partition leases, gRPC endpoints
- **S3 Storage**: Durable event storage (segment-based)
- **PostgreSQL/SQLite**: Metadata coordination (topics, partitions, offsets, consumer groups)
- **LRU Cache**: 10x database load reduction, segment caching

## Performance

### Producer (Phase 5.4)
| Metric | Value |
|--------|-------|
| Throughput | 62,325 records/sec |
| Batch size | 1000 records |
| Compression | LZ4 (30-50% reduction) |
| Latency (async) | < 1ms per send() |
| Offset tracking | < 200ns overhead |

### Consumer (Phase 6)
| Metric | Value |
|--------|-------|
| Throughput | 30,819 records/sec (debug), 100K+ expected (release) |
| Multi-partition | Round-robin fanout |
| Direct reads | Bypasses agents |
| Segment cache | 80% hit rate |

### Metadata & Storage
| Metric | Value |
|--------|-------|
| Metadata queries (cached) | < 100Âµs |
| Sequential reads | 3.10M records/sec |
| S3 durability | 99.999999999% |

## Development Status

| Phase | Status | Description | Tests | LOC |
|-------|--------|-------------|-------|-----|
| 1 | âœ… Complete | Core platform (S3 storage, SQLite metadata, gRPC API) | - | ~2000 |
| 2 | âœ… Complete | Performance optimizations (writer pooling, background flushing) | - | ~500 |
| 3.1 | âœ… Complete | Metadata abstraction | 26 | ~800 |
| 3.2 | âœ… Complete | PostgreSQL backend | - | ~400 |
| 3.3 | âœ… Complete | Metadata caching layer | - | ~300 |
| 4 | âœ… Complete | Multi-agent coordination | - | ~600 |
| 5.1 | âœ… Complete | Core client library (batching, retry, connection pooling) | 20 | ~650 |
| 5.2 | âœ… Complete | Producer implementation (agent discovery, partition routing) | 5 | ~800 |
| 5.3 | âœ… Complete | gRPC integration (compression, advanced tests) | 19 | ~610 |
| 5.4 | âœ… Complete | Producer offset tracking (async offset retrieval) | 3 | ~215 |
| 6 | âœ… Complete | Consumer API (multi-partition, consumer groups, offset management) | 8 | ~1005 |
| 7 | ðŸ“‹ Next | Observability (metrics, tracing, monitoring) | - | ~500 |
| 8 | ðŸ”„ Planned | Dynamic consumer group rebalancing | - | ~800 |
| 9 | ðŸ”„ Planned | Exactly-once semantics (transactions, idempotent producer) | - | ~1000 |

## Testing

```bash
# Run all tests
cargo test --workspace

# Run with PostgreSQL tests (requires running PostgreSQL)
DATABASE_URL=postgres://... cargo test --features postgres --workspace

# Run client tests (Producer + Consumer)
cargo test --release -p streamhouse-client

# Run specific integration tests
cargo test --package streamhouse-client --test producer_integration
cargo test --package streamhouse-client --test consumer_integration

# Run with output for benchmarks
cargo test --release -p streamhouse-client test_producer_throughput -- --nocapture
cargo test --release -p streamhouse-client test_consumer_throughput -- --nocapture
```

**Test Coverage**: 59 tests passing
- 20 unit tests (batch manager, retry policy, connection pool)
- 8 consumer integration tests
- 13 advanced gRPC integration tests
- 6 basic gRPC integration tests
- 5 producer integration tests
- 4 connection pool tests
- 3 offset tracking tests

## Contributing

StreamHouse is currently in active development. Contributions welcome!

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Run tests (`cargo test --workspace`)
4. Format code (`cargo fmt`)
5. Check with clippy (`cargo clippy -- -D warnings`)
6. Commit your changes (`git commit -m 'Add amazing feature'`)
7. Push to the branch (`git push origin feature/amazing-feature`)
8. Open a Pull Request

## License

MIT License - see [LICENSE](LICENSE) file for details

## Acknowledgments

- Inspired by Apache Kafka and WarpStream
- Built with Rust, Tokio, gRPC, PostgreSQL, and S3
- Thanks to the open source community

---

**Version**: v0.3.0 (Phases 1-6 Complete)
**Status**: Production Ready - Complete Producer and Consumer APIs
**Performance**: 62K writes/sec, 30K+ reads/sec, 59 tests passing
